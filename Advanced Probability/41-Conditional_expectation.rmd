# Conditioning

## Conditional Expectations

Throughout, $X$ is an $\overline{\mathbb{R}}$-valued $r.v.$ in $(\Omega,\mathcal{A},\mathop{{}\mathbb{P}})$. Let $\mathcal{F}$ be sub-$\sigma$-algebra, and we regard it as a body of information. And the conditional expectation $\mathop{{}\mathbb{E}}_{\mathcal{F}}X=\mathop{{}\mathbb{E}}(X|\mathcal{F})=\overline{X}$ is the "best" estimate of $X(\omega)$ given $\mathcal{F}$. Where the meaning of "best" comes from the usual MSE metric, $\mathop{{}\mathbb{E}}(X-\overline{X})^2$, which justified when $X$ is square integrable.

### Preparatory steps

Recall that $\mathcal{A}$ is the collection of all $\overline{\mathbb{R}}$-valued $r.v.'s$, similarly, $\mathcal{F}$ is the collection of all $\mathcal{F}$ measurable $r.v.'s$ taking value in $\overline{\mathbb{R}}$. WLOG, we assume $X \in \mathcal{A}_+$ for simplifying discussion.

Let $A$ be an event containing $\omega$, all we know about $\omega$ is it's in $A$. Based on this information, our best estimate should be simply the average over $A$:
$$
\mathop{{}\mathbb{E}}_H X=\frac{1}{\mathop{{}\mathbb{P}}H} \int _{H}Xd\mathop{{}\mathbb{P}}=\frac{\mathop{{}\mathbb{E}}X \bm{\mathbf{1}}_{H}}{\mathop{{}\mathbb{P}}H}
$$
where we assume $\mathop{{}\mathbb{P}}H>0$. Such definition is agree with $\mathop{{}\mathbb{E}}X$ when $A=\Omega$. The number $\mathop{{}\mathbb{E}}_H X$ is called **conditional expectation** of $X$ **given the event** $A$.

Next, suppose $\mathcal{F}$ is generated by a countable measurable partition $(A_{i})_{i \in \mathbb{N}^*}$, that is, we know $\omega$ located in which one of them, thus
$$
\mathop{{}\mathbb{E}}_{\mathcal{F}}X(\omega)=\sum_{n}^{} (\mathop{{}\mathbb{E}}_{A_n}X)\bm{\mathbf{1}}_{A_n}(\omega)
$$
Where we regard $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ as a $r.v.$ $\overline{X}$ and called **conditional expectation** of $X$ given $\mathcal{F}$.

To proceed to general $\mathcal{F}$, note that:

1.  $\overline{X} \in \mathcal{F}$.
2.  $\mathop{{}\mathbb{E}}VX=\mathop{{}\mathbb{E}}V \overline{X}$ for every $V\in \mathcal{F}_+$. Which follows by noting $V$ of the form $V=\sum_{}^{}a_n \bm{\mathbf{1}}_{A_n}$.


### Definition of conditional expectations


::: {.definition  name=""}

Let $\mathcal{F}\subset \mathcal{A}$, the **conditional expectation** of $X$ given $\mathcal{F}$, denoted by $\mathop{{}\mathbb{E}}_{\mathcal{F}}H$ is:

1.  If $X \in \mathcal{A}_+$. $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ is some $r.v.$ $\overline{X}$ $s.t.$:

    1. $\overline{X} \in \mathcal{F}_+$
    2. $\mathop{{}\mathbb{E}}VX=\mathop{{}\mathbb{E}}V\overline{X}$

    And them we write $\mathop{{}\mathbb{E}}_{\mathcal{F}}X=\overline{X}$ and call $\overline{X}$ a version of $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$.
2.  For arbitrary $X$, if $\mathop{{}\mathbb{E}}X$ exists, define:
    $$
    \mathop{{}\mathbb{E}}_{\mathcal{F}}X=\mathop{{}\mathbb{E}}_{\mathcal{F}}X^{+}-\mathop{{}\mathbb{E}}_{\mathcal{F}}X^{-}
    $$

3.  Otherwise, $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ is undefined, it's reasonable since so is $\mathop{{}\mathbb{E}}X$.

:::


::: {.remark}

1.  The projection property is equivalent to $\mathop{{}\mathbb{E}}X \bm{\mathbf{1}}_{A}=\mathop{{}\mathbb{E}}\overline{X}\bm{\mathbf{1}}_{A}$ for any $A\in \mathcal{F}$.
2.  $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ is unique up to equivalence.
3.  Note $\mathop{{}\mathbb{E}}X=\mathop{{}\mathbb{E}}\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ by letting $V=1$, thus $X$ is integrable iff so is $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$. In which case, the projection property can be expressed as: for every $V\in \mathcal{F}_b$, $\mathop{{}\mathbb{E}}V(X-\overline{X})=0$.
4.  Suppose $X$ is integrable, then so is $\overline{X}$ and $\widetilde{X}=X-\overline{X}$. Then we have decomposition
    $$
    X=\overline{X}+\widetilde{X}
    $$
    where $\widetilde{X}$ is orthogonal to $\mathcal{F}$ $s.t.$ $\mathop{{}\mathbb{E}}\widetilde{X}\bm{\mathbf{1}}_{A}=0$ for all $A\in \mathcal{F}$ and that's why we named "projection property".

:::

### Existence of conditional expectation

The following uses the Radon-Nikodym theorem to show the existence of conditional expectations. WLOG, we assume $X$ is positive.


::: {.theorem  name=""}

Let $X\in \mathcal{A}_+$ and $\mathcal{F}\subset \mathcal{A}$, then $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ exists and unique up to equivalence.

:::


::: {.proof}

$\forall A\in \mathcal{F}$, define
$$
P(A)=\mathop{{}\mathbb{P}}A,Q(A)=\int_{A}Xd\mathop{{}\mathbb{P}}=\mathop{{}\mathbb{E}}X \bm{\mathbf{1}}_{A}
$$
then $P$ and $Q$ are measures on $(\Omega,\mathcal{F})$ where $Q\ll P$ and we have $dQ=Xd\mathop{{}\mathbb{P}}$ by proposition \@ref(prp:indefinite-integral). Hence, by the Radon-Nikodym theorem \@ref(thm:radon), there exists $\overline{X}\in \mathcal{F}_+$ $s.t.$
$$
\mathop{{}\mathbb{E}}VX=\int_{\Omega}VdQ=\int _{\Omega}V\overline{X} dP=\mathop{{}\mathbb{E}}V \overline{X}
$$
for every $V\in \mathcal{F}_+$. Thus the claim follows.

:::

### Examples

::: {.example #perfect-information name=""}

Suppose our information is perfect, that is $X\in \mathcal{F}$, then $\mathop{{}\mathbb{E}}_{\mathcal{F}}X=X$ clearly.

:::


::: {.example #no-information name=""}

The other extreme case is $\mathcal{F}$ is independent with $X$. We claim that, $\mathop{{}\mathbb{E}}_{\mathcal{F}}X=\mathop{{}\mathbb{E}}_{}X$. Check:

1.  $\mathop{{}\mathbb{E}}_{}X$ is a constant and thus $\mathcal{F}$-measurable clearly.
2.  Note $\bm{\mathbf{1}}_{A}$ and $X$ are independent for $A\in \mathcal{F}$, then
    $$
    \mathop{{}\mathbb{E}}_{}X \bm{\mathbf{1}}_{A}=\mathop{{}\mathbb{E}}_{}X \mathop{{}\mathbb{E}}_{}\bm{\mathbf{1}}_{A}=\mathop{{}\mathbb{E}}_{}(\mathop{{}\mathbb{E}}_{}X \bm{\mathbf{1}}_{A})
    $$

:::


::: {.example  name=""}

Suppose $(\Omega_{i})_{i \in \mathbb{N}^*}$ is a at most countable partition of $\Omega$ where each has positive probability. Let $\mathcal{F}$ be $\sigma$-algebra generated by which. In the beginning of this chapter, we have
$$
\mathop{{}\mathbb{E}}_{\mathcal{F}}X=\mathop{{}\mathbb{E}}_{\Omega_i} X=\frac{\mathop{{}\mathbb{E}}_{}X \bm{\mathbf{1}}_{\Omega_i}}{\mathop{{}\mathbb{P}}\Omega_i}
$$
where $\Omega_i$ is where the outcome located.

Let $\mathop{{}\mathbb{P}}_{\mathcal{G}}A=\mathop{{}\mathbb{P}}(A|\mathcal{G})=\mathop{{}\mathbb{E}}_{\mathcal{G}}\bm{\mathbf{1}}_{A}$ and $\mathop{{}\mathbb{P}}_{B}A=\mathop{{}\mathbb{P}}(A|B)=\mathop{{}\mathbb{E}}_{B}\bm{\mathbf{1}}_{A}$. Then
$$
\mathop{{}\mathbb{P}}B\mathop{{}\mathbb{P}}(A|B)=\mathop{{}\mathbb{P}}B\mathop{{}\mathbb{E}}_{B}\bm{\mathbf{1}}_{A}=\mathop{{}\mathbb{E}}_{} \bm{\mathbf{1}}_{A} \bm{\mathbf{1}}_{B}=\mathop{{}\mathbb{E}}_{}\bm{\mathbf{1}}_{A\cap B}=\mathop{{}\mathbb{P}}(A\cap B)
$$
and We have $\mathop{{}\mathbb{P}}(X|\mathcal{F})=\mathop{{}\mathbb{P}}(X|\Omega_i)$.

:::


::: {.example #undergraduate-probability name=""}

Suppose $X,Y$ have joint distribution $f(x,y)$, in undergraduate probability, we have
$$
\mathop{{}\mathbb{E}}_{}(g(X)|Y=y)=\int g(x) \frac{f(x,y)}{\int f(x,y)dx}dx
$$
Now we show that:
$$
\mathop{{}\mathbb{E}}_{\sigma(Y)}g(X)=\mathop{{}\mathbb{E}}_{}(g(X)|Y)
$$
Clearly, $\mathop{{}\mathbb{E}}_{}(g(X)|Y)\in \sigma(Y)$. For the projection properties, note if $A\in \sigma(Y)$ then $A=Y^{-1}(B)$ for some $B\in \mathcal{B}$, thus
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}_{}\left[ (\mathop{{}\mathbb{E}}_{}g(X)|Y)\bm{\mathbf{1}}_{A} \right]&=\int_{B}\int g(x) \frac{f(x,y)}{f_{Y}(y)}dxf_{Y}(y)dy
    \\ &= 
    \int _{B} \int g(x)f(x,y) dxdy
    \\ &= 
    \mathop{{}\mathbb{E}}_{}g(X)\bm{\mathbf{1}}_{Y^{-1}(B)}=\mathop{{}\mathbb{E}}_{}g(X)\bm{\mathbf{1}}_{A}
\end{aligned}
$$

:::


### Properties of conditional expectations

#### Similar to expectations

::: {.proposition #conditional-expectations-properties name=""}

Assume all conditional expectations exists and $a,b,c \in \mathbb{R}$. Of course, all these conditional
expectations exist if all the random variables are positive or integrable.

-   Monotonicity: $X\le Y \implies \mathop{{}\mathbb{E}}_{\mathcal{F}}X \le \mathop{{}\mathbb{E}}_{\mathcal{F}}Y$
-   Linearity: $\mathop{{}\mathbb{E}}_{\mathcal{F}}(aX+bY+c)=a\mathop{{}\mathbb{E}}_{\mathcal{F}}X+b\mathop{{}\mathbb{E}}_{\mathcal{F}}Y+c$
-   MCT: $X_n\ge 0,X_n \nearrow X\implies \mathop{{}\mathbb{E}}_{\mathcal{F}}X_n \nearrow \mathop{{}\mathbb{E}}_{\mathcal{F}}X$
-   Fatou's lemma: $X_n\ge 0\implies\mathop{{}\mathbb{E}}_{\mathcal{F}}\liminf X_n \le \liminf \mathop{{}\mathbb{E}}_{\mathcal{F}}X_n$
-   DCT: Suppose $X_n \to X$ and bounded by some integrable $Y$, then $\mathop{{}\mathbb{E}}_{\mathcal{F}}X_n\to \mathop{{}\mathbb{E}}_{\mathcal{F}}X$
-   Jensen's inequality: Suppose $f$ is convex, then $\mathop{{}\mathbb{E}}_{\mathcal{F}}f(X)\ge f(\mathop{{}\mathbb{E}}_{F}X)$

:::

::: {.proof}

**Monotonicity**. Note $X\le Y$ implies $\mathop{{}\mathbb{E}}_{}X \bm{\mathbf{1}}_{A}\le \mathop{{}\mathbb{E}}_{}Y \bm{\mathbf{1}}_{A}$ and that is $\mathop{{}\mathbb{E}}_{}\overline{X}\bm{\mathbf{1}}_{A}\le \mathop{{}\mathbb{E}}_{}\overline{Y}\bm{\mathbf{1}}_{A}$ and claim follows.

**Linearity**. Clearly the right side is $\mathcal{F}$ measurable and it's remain to show it's projection property. Then claim follows from linearity of integral and the projection property of $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ and $\mathop{{}\mathbb{E}}_{\mathcal{F}}Y$ by checking $V=\bm{\mathbf{1}}_{A}$ for $A\in \mathcal{F}$.

**MCT**. Suppose $\overline{X}_n \nearrow \overline{X}$ where $\overline{X}_n$ is version of $\mathop{{}\mathbb{E}}_{\mathcal{F}}X_n$ respectively. Then it's remain to check $\overline{X}$ is version of $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$. It's $\mathcal{F}$-measurable clearly and for $V\in \mathcal{F}_+$:
$$
\mathop{{}\mathbb{E}}_{} V\overline{X}=\lim_{n \to \infty}\mathop{{}\mathbb{E}}_{}V\overline{X}_n=\lim_{n \to \infty}\mathop{{}\mathbb{E}}_{}VX_n=\mathop{{}\mathbb{E}}_{}VX
$$
thus the claim follows.

:::





#### Special properties



::: {.proposition #special-conditional-properties name=""}

Let $\mathcal{F},\mathcal{G}\subset \mathcal{A}$ and $X,W \in \mathcal{A}$ $s.t.$ $\mathop{{}\mathbb{E}}_{}X$ and $\mathop{{}\mathbb{E}}_{}WX$ integrable, then

1. Conditional determinism: $W\in \mathcal{F}\implies \mathop{{}\mathbb{E}}_{\mathcal{F}}WX=W\mathop{{}\mathbb{E}}_{\mathcal{F}}X$.
2. Repeated conditioning: $\mathcal{F}\subset \mathcal{G}\implies\mathop{{}\mathbb{E}}_{\mathcal{F}}\mathop{{}\mathbb{E}}_{\mathcal{G}}X=\mathop{{}\mathbb{E}}_{\mathcal{G}}\mathop{{}\mathbb{E}}_{\mathcal{F}}X=\mathop{{}\mathbb{E}}_{\mathcal{F}}X$.

:::


::: {.proof}

**Conditional determinism**. WLOG, Suppose $W$ is positive, then we have, for any $V\in \mathcal{F}_{+}$:
$$
\mathop{{}\mathbb{E}}_{}V(WX)=\mathop{{}\mathbb{E}}_{}(VW)X=\mathop{{}\mathbb{E}}_{}(VW)\overline{X}=\mathop{{}\mathbb{E}}_{}V(W\overline{X})
$$
as $VW \in \mathcal{F}_{+}$. It follows that $W\overline{X}=W\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ is a version of $\mathop{{}\mathbb{E}}_{\mathcal{F}}WX$.


**Repeated conditioning**. $\mathop{{}\mathbb{E}}_{\mathcal{G}}\mathop{{}\mathbb{E}}_{\mathcal{F}}X=\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ as example \@ref(exm:perfect-information) by noting $\mathop{{}\mathbb{E}}_{\mathcal{F}}X\in \mathcal{F} \subset \mathcal{G}$. For the other side, note $\mathop{{}\mathbb{E}}_{\mathcal{F}}X\in \mathcal{F}$, for $A\in \mathcal{F}\subset G$:
$$
\mathop{{}\mathbb{E}}_{}\left[ \bm{\mathbf{1}}_{A}\mathop{{}\mathbb{E}}_{\mathcal{F}}X \right]=\mathop{{}\mathbb{E}}_{}\bm{\mathbf{1}}_{A}X=\mathop{{}\mathbb{E}}_{}\left[ \bm{\mathbf{1}}_{A}\mathop{{}\mathbb{E}}_{\mathcal{G}}X \right]
$$
by the projection properties of $\mathop{{}\mathbb{E}}_{\mathcal{G}}X$ and thus meet the projection properties of $\mathop{{}\mathbb{E}}_{\mathcal{F}}(\mathop{{}\mathbb{E}}_{\mathcal{G}}X)$.

:::


::: {.remark}

For the repeated conditioning, think of $\mathcal{F}$ as the information a fool has, and $\mathcal{G}$ as that a genius has: the genius cannot improve on the fool's estimate, but the fool has no difficulty worsening the genius's. In repeated conditioning, fools win all the time.

:::


::: {.corollary name=""}

If $\mathcal{F}\subset \mathcal{G}$ and $\mathop{{}\mathbb{E}}_{\mathcal{G}}X\in \mathcal{F}$ then $\mathop{{}\mathbb{E}}_{\mathcal{G}}X$ is a version of $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$.

:::

::: {.proof}

$\mathop{{}\mathbb{E}}_{\mathcal{G}}X \in \mathcal{}\mathcal{F}$ implies $\mathop{{}\mathbb{E}}_{\mathcal{F}}\mathop{{}\mathbb{E}}_{\mathcal{G}}X=\mathop{{}\mathbb{E}}_{\mathcal{G}}X$, which also equivalent to $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ by repeated conditioning.

:::


### Conditioning as projection

Recall we interpret $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ as a projection which minimize the MSE of $X$.


::: {.theorem  name=""}

For every $X\in L^2(\mathcal{A})$, $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$ minimize $\mathop{{}\mathbb{E}}_{}\left|X-Y\right|^2$.

:::


::: {.remark}

$\overline{X}$ is the orthogonal projection of $X$ onto $L^2(\mathcal{F})$ and we have decomposition $X=\overline{X}+\widetilde{X}$ where $\overline{X} \in L^2(\mathcal{F})$ and $\widetilde{X} \perp L^2(\mathcal{F})$.

:::


::: {.proof}

Let $Z=\mathop{{}\mathbb{E}}_{\mathcal{F}}X-Y$ and $Y\in L^2(\mathcal{F})$ then:
$$
\mathop{{}\mathbb{E}}_{}(X-Y)^2=\mathop{{}\mathbb{E}}_{}(X-\mathop{{}\mathbb{E}}_{\mathcal{F}}X+Z)^2
$$
It's sufficient to show that is $\mathop{{}\mathbb{E}}_{}(X-\mathop{{}\mathbb{E}}_{\mathcal{F}}X)^2+\mathop{{}\mathbb{E}}_{}Z^2$. Which motivated us to consider cross-product term:
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}_{}\left[ Z(X-\mathop{{}\mathbb{E}}_{\mathcal{F}}X) \right]&=
    \mathop{{}\mathbb{E}}_{}ZX-\mathop{{}\mathbb{E}}_{}(Z\mathop{{}\mathbb{E}}_{\mathcal{F}}X)
    \\ &= 
    \mathop{{}\mathbb{E}}_{}ZX-\mathop{{}\mathbb{E}}_{}(\mathop{{}\mathbb{E}}_{\mathcal{F}}ZX)
    \\ &= 
    \mathop{{}\mathbb{E}}_{}ZX-\mathop{{}\mathbb{E}}_{}ZX=0
\end{aligned}
$$

:::

### Conditional expectations given $r.v.'s$

Now we extend the definition in example \@ref(exm:undergraduate-probability). If $\{Y_{t}:t\in T\}$ is a collection of $r.v.'s$, then denoted $\mathcal{F}=\sigma \{Y_t:t\in T\}$ as usual. Then the conditional expectations given $\{Y_{t}:t\in T\}$ is just $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$.

In the light of Doob-Dynkin theorem \@ref(thm:doob-dynkin), we have $\mathop{{}\mathbb{E}}_{\sigma Y}X$ has the form $f\circ Y$.

## Conditional probability and distribution

Recall definition in example \@ref(exm:undergraduate-probability):
$$
\mathop{{}\mathbb{P}}_{\mathcal{F}}H=\mathop{{}\mathbb{E}}_{\mathcal{F}}\bm{\mathbf{1}}_{H}=\begin{cases}
    \bm{\mathbf{1}}_{H} & H\in \mathcal{F} \\
    \mathop{{}\mathbb{P}}H& H \perp \mathcal{F}
\end{cases}
$$

### Regular versions

Let $Q(A)$ be a version of $\mathop{{}\mathbb{P}}_{\mathcal{F}}A$ for each $A\in \mathcal{A}$. Clearly, $Q(\emptyset)=0$ and $Q(\Omega)=1$. Let $Q(\omega,A)=Q(A)(\omega)=Q_{\omega}(A)$, note

-   $\omega \mapsto Q(\omega,A)=Q(A)$ is $\mathcal{F}$ measurable.
-   $A \mapsto Q(\omega,A)=Q_{\omega}(A)$ is a measure:

    1.  Nonnegativity: $Q_{\omega}(A) \ge 0$ for any $A$ clearly and
    1.  $\sigma$-additivity:
        $$
        Q_{\omega}\left( \sum_{n}A_n \right)=\mathop{{}\mathbb{E}}_{\mathcal{F}}\bm{\mathbf{1}}_{\sum_{n}A_n}=\mathop{{}\mathbb{E}}_{\mathcal{F}}\sum_{n}^{} \bm{\mathbf{1}}_{A_n}=\sum_{n}^{}\mathop{{}\mathbb{E}}_{\mathcal{F}}\bm{\mathbf{1}}_{A_n}=\sum_{n}^{}Q_{\omega}(A_n)
        $$

However, the $\sigma$-additivity only enjoyed in a $a.s.$ $\Omega_0$ (as we use the MCT of conditional expectations which works on a $a.s.$ set) and that keeps $Q$ from being a transition kernel. Suppose $\Omega_{a}$ be the $a.s.$ event $w.r.t.$ sequence $a=(A_n)$. Then we need $\bigcap_{a}\Omega_a$ to be $a.s.$ and this usually be a miserable object as there are uncountable many sequence $a$.

Nevertheless, it's often possible to pick versions of $Q(A)$ $s.t.$ $\bigcap_{a}\Omega_{a}=\Omega$.


::: {.definition  name=""}

$Q(\omega,A)$ is said to be a **regular version** of $\mathop{{}\mathbb{P}}_{\mathcal{F}}$ or a **regular conditional probability** provided that $Q$ is a transition probability kernel from $(\Omega,\mathcal{F})$ into $(\Omega,\mathcal{A})$.

:::

The following is the reason for our interesting in regular version.


::: {.proposition  name=""}

Suppose $\mathop{{}\mathbb{P}}_{\mathcal{F}}$ has a regular version $Q$, then
$$
QX(\omega)=Q_{\omega}X=\int X dQ_{\omega}
$$
is a version of $\mathop{{}\mathbb{E}}_{\mathcal{F}}X$.

:::


::: {.proof}

WLOG, assume $X\in \mathcal{A}_+$. By theorem \@ref(thm:kernel-op), we have $QX\in \mathcal{F}_+$, then it's sufficient to check the projection property, to see this, suppose $X=\bm{\mathbf{1}}_{A}$ for arbitrary $A\in \mathcal{A}$, then
$$
\mathop{{}\mathbb{E}}_{}VQX=\mathop{{}\mathbb{E}}_{}VQ \bm{\mathbf{1}}_{A}=\mathop{{}\mathbb{E}}_{}VQ(A)=\mathop{{}\mathbb{E}}_{}V \bm{\mathbf{1}}_{A}=\mathop{{}\mathbb{E}}_{}VX
$$
then we can extends $X$ to general case and claim follows.

:::

The existence of regular version for $\mathop{{}\mathbb{P}}_{\mathcal{F}}$ require conditions either on $\mathcal{F}$ or $\mathcal{A}$.

-   $\mathcal{F}$ generated by a measurable partition $(\Omega_n)$ of $\Omega$, then
    $$
    Q_{\omega}(A)=\sum_{n}^{} \mathop{{}\mathbb{P}}_{\Omega_n}A \cdot \bm{\mathbf{1}}_{\Omega_n}(\omega)=\mathop{{}\mathbb{P}}_{\Omega_i}A
    $$
    where $\Omega_i$ is where $\omega$ located and thus be a measure as so is $\mathop{{}\mathbb{P}}_{\Omega_i}$.






<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>


