# Probability Spaces

## Probability Spaces and Random Variables

::: {.definition  name=""}

Let $(\Omega,\mathcal{F},\mathop{{}\mathbb{P}})$ be a probability space. The set $\Omega$ is called the **sample space** and whose elements are called **outcomes**. $\mathcal{F}$ is called **history** and whose elements are called **events**.

:::

Note here $\mathop{{}\mathbb{P}}$ is finite measure, so it's continuous. We collect it's properties below :


::: {.proposition  name=""}

For probability measure, which has following properties:

1. $\forall A\in \mathcal{A},\quad 0\le \mathop{{}\mathbb{P}}(A) \le 1$
2. $\mathop{{}\mathbb{P}}(\Omega)=1$
3. $\mathop{{}\mathbb{P}} \left( \sum _ { 1 } ^ { \infty } A _ { n } \right) = \sum _ { 1 } ^ { \infty } \mathop{{}\mathbb{P}} \left( A _ { n } \right)$
4. $\mathop{{}\mathbb{P}}(A) \le \mathop{{}\mathbb{P}}(B) \impliedby A\sub B$
5. $\mathop{{}\mathbb{P}}$ is continuous, as well as continuous from above and below.

6.  **Boole's inequality**
    $$
    \mathop{{}\mathbb{P}} \left( \bigcup _ { i = 1 } ^ { \infty } A _ { i } \right) \leq \sum _ { i = 1 } ^ { \infty } \mathop{{}\mathbb{P}} \left( A _ { i } \right)
    $$

:::

### Measure-theoretic and probabilistic languages

|    Measure     |   Probability   |
| :------------: | :-------------: |
|    Integral    |   Expectation   |
| Measurable set |      Event      |
| Measurable function | Random Variable |
|      a.e.      |      a.s.       |

## Distribution

Let $X$ be a r.v. taking values in some measurable space $(Y,\mathcal{Y})$, then let $\mu$ be the image of $\mathop{{}\mathbb{P}}$ under $X$, i.e.:
$$
\mu(A)=\mathop{{}\mathbb{P}}(X^{-1}A)=\mathop{{}\mathbb{P}}\{X \in A\}
$$
then $\mu$ is a probability measure on $(Y,\mathcal{Y})$, it's called the **distribution** of $X$. In view of theorem \@ref(thm:measure-agree), it suffices to specify $\mu(A)$ forall $A$ belongs to a $\pi$-system which generates $\mathcal{Y}$. In particular, if $(Y,\mathcal{Y})=(\overline{\mathbb{R}},\mathcal{B})$, it's enough to specify
$$
c(x)=\mu[-\infty,x]=\mathop{{}\mathbb{P}}\{X \le x\}
$$
when $x\in \mathbb{R}$, such $c:\mathbb{R} \to [0,1]$ is called **distribution function(d.f.)** and is nondecreasing and right continuous. As it's increasing and bounded, the one-side limit always exists. In fact, we have
$$
\begin{aligned}
    &c(-\infty)=\mathop{{}\mathbb{P}}\{X=-\infty\},c(\infty)=\mathop{{}\mathbb{P}}\{X=\infty\}
    \\&
    c(x-)=\mathop{{}\mathbb{P}}\{X<x\},c(x)-c(x-)=\mathop{{}\mathbb{P}}\{X=x\}
\end{aligned}
$$
Let $D$ be set of all atoms in $\mu$, then $D$ is
$$
D=\{x\in \overline{\mathbb{R}}:\mathop{{}\mathbb{P}}\{X=x\}>0 \}
$$
and countable by lemma \@ref(lem:atomic-countable). Define $D_x=D\cap [-\infty,x]$ and
$$
a(x)=\sum_{y \in D_x}^{} \mathop{{}\mathbb{P}}\{X=y\},b(x)=c(x)-a(x)
$$
for $x\in \mathbb{R}$. Then $a$ is d.f. of measure $\mu_a$ defined by
$$
\mu_a(B)=\mu(B \cap D),B\in \mathcal{B}(\overline{\mathbb{R}})
$$
and $b$ is d.f. of $\mu_{b}=\mu-\mu_a$. Then we decomposition $\mu$ into a purely atomic $\mu_a$ and diffuse $\mu_b$.

### Quantile functions

Let $q:(0,1)\to \overline{\mathbb{R}}$ be the right continuous functional inverse of $c$,i.e.,
$$
q(u)=\inf \{x\in \mathbb{R},u < c(x)\}
$$
Note that $q$ is real valued iff $c(\infty)=1$ and $c(-\infty)=0$.

As $c$ is right continuous, if $c(x)<u$, $q(u)>x$, but $c(x)>u$ can only implies $q(u)\le x$.

$c(x-)\le u$ iff $q(u)\ge x$ and $q(u-)\le x$ iff $c(x)\ge u$.

Let $\lambda$ denote the Lebesgue measure on $(0,1)$ then $\mu=\lambda \circ q^{-1}$.

### Joint distributions

Let $X$ and $Y$ taking values in $(E,\mathcal{E})$ and $(F, \mathcal{F})$ respectively then pair $Z=(X,Y)$ is measurable from $\mathcal{F}$ to $\mathcal{E \times F}$.

Recall the product spaces, to specifies distribution $\pi$ of $Z$ is suffices to: 
$$
\pi(A\times B)=\mathop{{}\mathbb{P}}\{X \in A, Y\in B\}
$$
thus we have
$$
\mu(A)=\mathop{{}\mathbb{P}}\{x\in A\}=\pi(A\times F)
$$
$\mu$ and $\nu$ are called **marginal distributions**


#### Independence of distribution

Let $X$ and $Y$ taking values in $(E,\mathcal{E})$ and $(F, \mathcal{F})$ with marginal $\mu$ and $\nu$, then they are said **independent** if their joint distribution is the product formed by their marginals:
$$
\mathop{{}\mathbb{P}}\{X\in A,Y\in B\}=\mathop{{}\mathbb{P}}\{X\in A\}\mathop{{}\mathbb{P}}\{Y\in B\}
$$
A finite collection $\{X_i\}_i^n$ is said to be **independency** if their product distribution has form $\prod_{i=1}^n \mu_i$. An arbitrary collection of r.v. is an independency if every finite subcollection is so.

## Expectation

Suppose $X$ taking values in $\overline{\mathbb{R}}$, then we can talk about it's expectation:
$$
\mathop{{}\mathbb{E}} X=\int_{\Omega} X d\mathop{{}\mathbb{P}}=\mathop{{}\mathbb{P}}X
$$
the integral of $X$ over an event $H \in \mathcal{F}$ is $\mathop{{}\mathbb{E}}X \bm{1}_H$

### Properties of expectation

Suppose $X,Y$ taking values in $\overline{\mathbb{R}}$ and $a,b>0$. The following holds:

(**Absolute integrability**). $\mathop{{}\mathbb{E}}X$ is finite iff $\mathop{{}\mathbb{E}}|X|$ is finite.

**(Positivity)** If $X\ge 0$ a.s., then $\mathop{{}\mathbb{E}}X\ge 0$

**(Monotonicity)** If $X\ge Y$ or either $\mathop{{}\mathbb{E}}X$ and $\mathop{{}\mathbb{E}} Y$ is finite then both $\mathop{{}\mathbb{E}}X$ and $\mathop{{}\mathbb{E}}Y$ exist and $\mathop{{}\mathbb{E}}X\ge \mathop{{}\mathbb{E}}Y$.

**(Linearity)** 
$$ \mathop{{}\mathbb{E}}(aX+bY)=a\mathop{{}\mathbb{E}}X+b\mathop{{}\mathbb{E}}Y $$

**($\sigma$ additivity over sets)** If $A=\sum_{i=1}^\infty A_i$, then
$$ \mathop{{}\mathbb{E}} _ { A } X = \sum _ { i = 1 } ^ { \infty } \mathop{{}\mathbb{E}} _ { A _ { i } } X $$

**(Mean value theorem)** If $a\le X \le b$ a.s., then 

$$ a\mathop{{}\mathbb{P}}(A)\le \mathop{{}\mathbb{E}}_AX\le b\mathop{{}\mathbb{P}}(A) $$

**(Modulus inequality)**: $|\mathop{{}\mathbb{E}}X|\le \mathop{{}\mathbb{E}}|X|$

**(Fatou's) inequality** If  $X_n\ge 0$ a.s., then
$$ \mathop{{}\mathbb{E}} \left( \liminf _ { n } X _ { n } \right) \leq \liminf _ { n } \mathop{{}\mathbb{E}} X _ { n } $$

**(Dominated Convergence Theorem)** If $X_n \to X$ a.s., $|X_n|\le Y$ a.s. for all n and $\mathop{{}\mathbb{E}}Y<\infty$, then

$$ \lim_n \mathop{{}\mathbb{E}}X_n=\mathop{{}\mathbb{E}}X=\mathop{{}\mathbb{E}}\lim_n X_n $$

**(Monotone Convergence Theorem)** If $0\le X_n\nearrow X$, then
$$ \lim_n \mathop{{}\mathbb{E}}X_n=\mathop{{}\mathbb{E}}X=\mathop{{}\mathbb{E}}\lim_n X_n $$

**(Integration term by term)** If $\sum_{i=1}^\infty \mathop{{}\mathbb{E}}|X_n|<\infty$, then
$$ \sum_{i=1}^\infty |X_n|<\infty,\ a.s. $$ 
and

$$ \mathop{{}\mathbb{E}} \left( \sum _{ i = 1 } ^ { \infty } X_ { n } \right) = \sum _{ i = 1 } ^ { \infty } \mathop{{}\mathbb{E}} X_ { n } $$ 



::: {.remark  name=""}

1. If $\mathop{{}\mathbb{P}}(A)=1$, then $\mathop{{}\mathbb{E}}X=\mathop{{}\mathbb{E}}_AX$.
2. If $\mathop{{}\mathbb{E}}|X|<\infty$, then $|X|<\infty$ a.s., but not vise versa.
3. If $X=Y$ a.s. and either $\mathop{{}\mathbb{E}}X$ or $\mathop{{}\mathbb{E}}Y$ exists, then so is the other and they are equal.
4. $\forall H\in \mathcal{F},\mathop{{}\mathbb{E}}X \bm{1}_H \ge \mathop{{}\mathbb{E}} Y \bm{1}_{H}\implies X \ge Y$ a.s. To see this, if there exist a subset $A\subset H$ s.t. $X<Y$ and $\mu(A)>0$ then there is a contradiction with monotonicity in $A$.

:::

### Expectations and integrals

The following relates expectation and integrals w.r.t. distribution.


::: {.theorem #expectation-distribution name=""}

If $X\ge 0$, then

$$
\mathop{{}\mathbb{E}}X=\int_{0}^{\infty}\mathop{{}\mathbb{P}}\{X>x\}dx
$$

:::


::: {.proof}


Note
$$
X(\omega)=\int_{0}^{X(\omega)} dx = \int_{0}^{\infty} \bm{1}_{X>x}(\omega)dx
$$
then
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}X&=\int_{\Omega}X(\omega)\mathop{{}\mathbb{P}}(d\omega)
    \\ &= 
    \int_{\Omega} \int_{0} ^{\infty}\bm{1}_{X>x}(\omega)dx\mathop{{}\mathbb{P}}(d\omega)
    \\ &= 
    \int_{0} ^{\infty}\int_{\Omega} \bm{1}_{X>x}(\omega)\mathop{{}\mathbb{P}}(d\omega)dx 
    \\ &= \int_{0} ^{\infty} \mathop{{}\mathbb{P}}\{X>x\}dx
\end{aligned}
$$
:::



::: {.theorem #expectation-integral name=""}

Let $X$ be a r.v. taking value in $(E, \mathcal{E})$ then
$$
\int f\circ X d\mathop{{}\mathbb{P}}=\mathop{{}\mathbb{E}}f\circ X=\mu f=\int fd\mu
$$
holds for all $f\in \mathcal{E}$ iff $\mu$ is the distribution of $X$.

:::


::: {.proof}

Note $\mu=\mathop{{}\mathbb{P}}\circ X^{-1}$, then $\impliedby$ follows from theorem \@ref(thm:image-measure). For $\implies$, taking $f=\bm{1}_{A}$:
$$
\mu(A)=\mu \bm{1}_{A}=\mathop{{}\mathbb{E}}\bm{1}_{A}\circ X=\int\bm{1}_{A}\circ Xd\mathop{{}\mathbb{P}}
$$
that implies $\mu=\mathop{{}\mathbb{P}}\circ X^{-1}$ and claim follows.

:::


::: {.remark}

By intuition, for a measure $\mu$ to be distribution of $X$ it suffices to test all $f=\bm{1}_{A}$ for $A \in \mathcal{E}$ or even $A\in \mathcal{C}$ where $\mathcal{C}$ is a $\pi$ system and generating $\mathcal{E}$.

:::



::: {.definition  name=""}

Let $X$ be a r.v. taking values in $\overline{\mathbb{R}}$ with distribution $\mu$, define

1. $r$th Moment: $\mathop{{}\mathbb{E}}X^r$
2. $r$th Absolute Moment: $\mathop{{}\mathbb{E}}|X|^r$
3. $r$th Central Moment: $\mathop{{}\mathbb{E}}(X-\mathop{{}\mathbb{E}}X)^r$
4. $r$th Absolute Central Moment: $\mathop{{}\mathbb{E}}|X-\mathop{{}\mathbb{E}}X|^r$

:::




