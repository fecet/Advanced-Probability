## Information and determinability

###  $\sigma$ algebra generated by r.v.

Let $\left\{ X _ { \lambda } , \lambda \in \Lambda \right\}$ is r.v.s on $(\Omega,\mathcal{A})$. Define
$$
\sigma \left\{ X _ { \lambda } , \lambda \in \Lambda \right\} : = \sigma \left( X _ { \lambda } \in B , B \in \mathcal { B } , \lambda \in \Lambda \right) = \sigma \left( X _ { \lambda } ^ { - 1 } ( \mathcal { B } ) , \lambda \in \Lambda \right) = \sigma \left( \cup _ { \lambda \in \Lambda } X _ { \lambda } ^ { - 1 } ( \mathcal { B } ) \right)
$$
which is called $\sigma$ algebra generated by $\left\{ X _ { \lambda } , \lambda \in \Lambda \right\}$, where $\Lambda$ is a index set which can be uncountable. 

For $\Lambda=\mathbb{N^+}$:

1. 
   $$ \begin{aligned} \sigma \left( X _ { i } \right) & = \sigma \left( X _ { i } ^ { - 1 } ( \mathcal { B } ) \right) = X _ { i } ^ { - 1 } ( \mathcal { B } ) = \left\{ X _ { i } \in \mathcal { B } \right\} \\ \sigma \left( X _ { 1 } , \ldots , X _ { n } \right) & = \sigma \left( \cup _ { i = 1 } ^ { n } X _ { i } ^ { - 1 } ( \mathcal { B } ) \right) = \sigma \left( \cup _ { i = 1 } ^ { n } \sigma \left( X _ { i } \right) \right) \end{aligned} $$
2. 
   $$ \begin{array} { c } \sigma \left( X _ { 1 } \right) \subset \sigma \left( X _ { 1 } , X _ { 2 } \right) \subset \ldots \ldots \subset \sigma \left( X _ { 1 } , \ldots , X _ { n } \right) \\ \sigma \left( X _ { 1 } , X _ { 2 } , \ldots . . \right) \supset \sigma \left( X _ { 2 } , X _ { 3 } , \ldots . . \right) \supset \ldots \ldots \supset \sigma \left( X _ { n } , X _ { n + 1 } , \ldots . . \right) \end{array} $$

3. $\bigcap_1^\infty \sigma(X_n,X_{n+1},\cdots)$ is the tail $\sigma$ algebra of $X_{1:}$

In view of \@ref(prp:factor-measurable):

::: {.proposition  name=""}

If $X=(X_{t})_{t\in T}$, then $\sigma X=\sigma \{X_t:t \in T\}$

:::


::: {.theorem  name=""}

Let $X$ be a r.v. taking values in space $(E, \mathcal{E})$. A mapping $V:\Omega \to \overline{\mathbb{R}}$ belongs to $\sigma X$ iff $V=f(X)$ for some $f\in \mathcal{E}$.

:::


::: {.proof}

$\impliedby$ is immediately as measurable functions of measurable functions are measurable.

$\implies$. Let $\mathcal{M}=\{V:V=f(X)\}$, then it's a monotone class and claim follows from theorem \@ref(thm:MCT-function).

:::

Putting $X=(X_1,X_2,\dots)$ lead to

::: {.corollary  name=""}

Suppose $(X_n)_{n \in \mathbb{N}^*}$ are all r.v., then $V:\Omega \to  \overline{\mathbb{R}}$ belongs to $\sigma \{X_n:n \in \mathbb{N}^*\}$ iff $V=f(X_1,X_2,\dots)$ for some $f\in \prod_{i \in \mathbb{N}^{*}} \mathcal{E}_i$.

:::

This can be generalized to uncountable case:


::: {.proposition  name=""}

Suppose $(X_t)_{t\in T}$ is family of r.v. then $V:\Omega \to \overline{\mathbb{R}}$ belongs to $\sigma \{X_t:t \in T\}$ iff there exist countable $(t_n)\subset T$ and a function $f\in \prod_{(t_n)} \mathcal{E}_{t_n}$ s.t. $V=f(X_{t_1},X_{t_2},\dots)$.

:::


::: {.definition  name=""}

Suppose $X$ and $Y$ are r.v., then we say $X$ **determines** $Y$ if $Y=f \circ X$ for some measurable $f$. $\sigma X$ is called **information** as it contains all determined variables w.r.t. $X$.

:::



### Filtrations


::: {.definition  name=""}

A filtration is a filter with a total inclusion order where elements are all $\sigma$-algebra and denoted as $\mathcal{F}=\{\mathcal{F}_{t}:t\in T\}$ where $\mathcal{F}_t\subset \mathcal{F}_t$ provided $s<t$.

:::

Our aim is to approximate eternal variables by known r.v.:


::: {.theorem #approx-bounded-rv name=""}

Let $\mathcal{F}=\{\mathcal{F_n:n \in \mathbb{N}}\}$ be a filtration and $\mathcal{F}_\infty=\sigma(\bigcup_{t \in T}\mathcal{F}_{t})=\bigcup_{t \in T}\mathcal{F}_t$. For bounded $V\in \mathcal{F}_\infty$ there are sequence of bounded $V_n \in \mathcal{F}_n,n \in \mathbb{N}$, s.t.:
$$
\lim_{n \to \infty} \mathop{{}\mathbb{E}} |V_n-V|=\lim_{n \to \infty}\mathop{{}\mathbb{E}}V_n-\mathop{{}\mathbb{E}}V=0
$$
:::


::: {.proof}

Let $\mathcal{M}_b\subset \mathcal{F}_{\infty}$ be collection of bounded variables can be approximated. It follows that $\mathcal{M}_b$ is a monotone class and claim follows from theorem \@ref(thm:MCT-function).

:::


## Independence

Suppose $(\Omega,\mathcal{A},\mathop{{}\mathbb{P}})$ be a probability space and let $(\mathcal{F}_i)_{i \in I}$ be a finite family sub-$\sigma$-algebra of $\mathcal{A}$, then $\{\mathcal{F}_i: i \in I\}$ is called **independency** if
$$
\mathop{{}\mathbb{E}} \prod_{i\in I} V_n = \prod_{i\in I} \mathop{{}\mathbb{E}} V_i
$$
for all positive $V_i \in \mathcal{F}_i$ respectively.

If $I$ is arbitrary, then $\{\mathcal{F}_t:t \in I\}$ is independency if every finite subset of it is so.

### Independence of $\sigma$-algebras


::: {.lemma #test-independency name=""}

Suppose $(\mathcal{F})_{i \in S}$ be a finite family of sub-$\sigma$-algebras, let $\mathcal{C}_i$ be a $\pi$-system that generates $\mathcal{F}_i$ respectively, then $\{\mathcal{F}_i:i \in I\}$ are independent iff:
$$
\mathop{{}\mathbb{P}}(\bigcap_{i \in  I}A_i)=\prod_{i \in I} \mathop{{}\mathbb{P}}(A_i)
$$
for any $A_i \in \mathcal{C_i}\cup \{\Omega\}$ respectively.

:::


::: {.proof}

$\implies$ is immediately by taking $V_i=\bm{\mathbf{1}}_{A_i}$. For $\impliedby$, clearly the equality holds for all $A_i \in \mathcal{F_i}$ respectively in view of theorem \@ref(thm:measure-agree). It follows that indicator r.v. are independent and we can extend to general $V_i$ by theorem \@ref(thm:simple-approx) and theorem \@ref(thm:MCT).

:::

### Independence of collection

::: {.proposition #independency-partition  name=""}

Every partition of independency is an independency: let $\{\mathcal{F_t}:t \in T\}$ be an independency and $(T_i)_1^{\infty}$ be a partition of $T$ then $\{\mathcal{F}_{T_i}\}_i^{\infty}$ is an independency.

:::


::: {.proof}

Let $\mathcal{C_i}$ be all events having the form $\bigcap_{S} A_s$ where $A_s \in \bigcup_{t \in T_i} \mathcal{F_t}$, then they are $\pi$-systems contains $\Omega$ and generates $\mathcal{F}_{T_i}$. Then $\{\mathcal{F}_{T_i}: 1\le i\}$ is an independency follows from lemma \@ref(lem:test-independency) and $\{\mathcal{F_t}:t\in T\}$ is an independency.

:::


A collection of objects are said to be pairwise independent if every pair of them is an independency. Though it's weaker than mutually independent, we can check independency by respected checking pairwise independency.


::: {.lemma  name=""}

Countable collection of sub-$\sigma$-algebras $\{\mathcal{F_i}\}_1^\infty$ are independent iff $\mathcal{F_{\{1\le i\le n\}}}$ and $\mathcal{F}_{n+1}$ are independent for all $n \ge 1$.

:::


::: {.proof}

$\implies$ is immediate from \@ref(prp:independency-partition). For $\impliedby$, let $\mathcal{G_n}=\sigma(\bigcup_{i}^{n }\mathcal{F_i})$ and $A_i\in \mathcal{F_i}$ respectively for $1\le i\le m$ note:
$$
\bigcap_{1}^{m-1} A_i \in \mathcal{G_{m-1}}
$$
thus we can repeat apply lemma \@ref(lem:test-independency) and finally get what we need for apply lemma \@ref(lem:test-independency).

:::

### Independence of r.v.'s


::: {.lemma name=""}

The r.v.'s $X_1,\dots,X_n$ are independent iff
$$
\mathop{{}\mathbb{E}} \prod_{i=1} ^{\infty} f_i \circ X_i=\prod_{i=1} ^{\infty} \mathop{{}\mathbb{E}}f_i \circ X_i
$$
for all $f_i \in \mathcal{E_i}$ respectively.

:::


::: {.proof}

Clearly from $f\circ X \in \sigma X$

:::

Let $\pi$ be joint distribution of $X_1,\dots,X_n$ and let $\mu_1,\dots,\mu_n$ be corresponding marginals. Then the equality becomes
$$
\int_{\prod_{i=1}^{n} E_i} \prod_{i=1}^{n} f_i(x_i) d\pi=\prod_{i=1} ^{n} \int_{E_i} f_i(x_i) d\mu_i
$$
and that suggests $\pi=\prod_{i=1} ^{n} \mu_i$.


::: {.proposition  name=""}

The random variables $X_1,\dots,X_n$ are independent iff their joint distribution is the product of their marginal distributions.

:::

In view of determined variables are in $\sigma X$, we have

::: {.proposition  name=""}

Measurable functions of independent r.v.'s are independent.

:::

### Sum of independent r.v.'s

Let real valued r.v.'s $X$ and $Y$ with distribution $\mu$ and $\nu$ are independent. The distribution of $X+Y$ denoted as $\mu * \nu$ and given by
$$
(\mu * \nu) f=\mathop{{}\mathbb{E}}f(X+Y)=\iint f(x+y) d\nu d\mu
$$
This distribution $\mu * \nu$ is called **convolution** and can be extend to any number of distributions easily.

### Kolmogorov's $0$-$1$ law


::: {.definition  name=""}

Let $(\mathcal{G_n})$ be a sequence of sub-$\sigma$-algebras. We may treat $\mathcal{G_n}$ as the information revealed by the $n$th trial of an experiment. Then $\mathcal{J_n}=\sigma(\bigcup_{m>n}\mathcal{G_m})$ is information after $n$ and $\mathcal{J}=\bigcap_{n}\mathcal{J_n}$ is that about **remote future** and called **tail-$\sigma$-algebra**.

The sets of which are called **tail events**, and functions on which are **tail functions**.

:::


::: {.theorem #kolmogorov-0-1 name="Kolmogorov's 0-1 law"}

Tail events of independent $(\mathcal{G_i})_{1}^{\infty}$ have probability $0$ or $1$.

:::


::: {.proof}

By proposition \@ref(prp:independency-partition), $\{\mathcal{G_i}\}_{1}^{n}\cup \{\mathcal{J_n}\}$ is independency for each $n$ which implies so is $\{\mathcal{G_i}\}_{1}^{n}\cup \{\mathcal{J}\}$ as $\mathcal{J}\subset \mathcal{J_n}$ and thus so is $\{\mathcal{G_i}\}_{1}^{\infty}\cup \{\mathcal{J}\}$ by definition, that implies $\left\{ \mathcal{J},\mathcal{J_0} \right\}$ is an independency by proposition \@ref(prp:independency-partition) again and so is $\left\{ \mathcal{J,J} \right\}$ by noting $\mathcal{J}\subset \mathcal{J_0}$. Finally, for any event $A\in \mathcal{J}$, we have:
$$
\mathop{{}\mathbb{P}}(A\cap A)=\mathop{{}\mathbb{P}}(A)^2\implies \mathop{{}\mathbb{P}}(A)=0 \text{ or } 1
$$
as lemma \@ref(lem:test-independency).

:::


::: {.corollary  name=""}

Tail function of independent r.v.'s are degenerate a.s.

:::


::: {.proof}


Note that $Y\le c$ is tail events.

:::

By above corollary, we can see that $\limsup_n X_n$ and $\liminf_n X_n$ are degenerate a.s.

### Hewitt-Savage $0$-$1$ law

::: {.definition  name=""}

A **finite permutation** of $\mathbb{N}$ is a map $\pi:\mathbb{N}\to \mathbb{N}$ s.t. $\pi(n)=n$ for all but finite exception. For such permutation $\pi$, we write
$$
X \circ \pi = \{X_{\pi(i)}:i \in \mathbb{N}\}
$$
for countable $X=(X_1,X_2,\dots)$. Variable is said to be permutation invariant if $V\circ \pi=V$ for any $\pi$ and event is said to be so if its indicator is such.

:::

The collection of all permutation invariant events is a $\sigma$-algebra which contains the tail-$\sigma$-algebra of $X$. 

The following theorem generalized kolmogorov 0-1 law \@ref(thm:kolmogorov-0-1) in i.i.d. cases.


::: {.theorem #hawitt-savage name="Hewitt-Savage 0-1 law"}

Suppose $(X_i)_{i \in \mathbb{N}}$ are i.i.d., then every permutation invariant event has probability $0$ or $1$ and every permutation invariant r.v. is degenerate a.s..

:::


::: {.proof}

It's sufficient to show that if $V:\Omega \to [0,1]$ is permutation invariant in $\mathcal{F}_\infty$, then $\mathop{\text{Var}}\left[ V \right]=\mathop{{}\mathbb{E}}V^2-(\mathop{{}\mathbb{E}}V)^2=0$. For such $V$, there exist $\{V_n:n \in \mathbb{N}\}$ and also bounded in $[0,1]$ by theorem \@ref(thm:approx-bounded-rv) s.t.:
$$
\lim_{n \to \infty} \mathop{{}\mathbb{E}}|V-V_n|=\lim_{n \to \infty}\mathop{{}\mathbb{E}}V_n-\mathop{{}\mathbb{E}}V=0
$$
As $(X_i)_{i \in \mathbb{N}}$ are i.i.d. $V$ and $V \circ \pi$ share the same distribution and thus same expectation:
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}|V-V_n|&=\mathop{{}\mathbb{E}}|(V-V_n)\circ \pi|
    \\ &= 
    \mathop{{}\mathbb{E}}|V\circ \pi-V_n \circ \pi|
    \\ &= 
    \mathop{{}\mathbb{E}}|V-V_n \circ \pi|
\end{aligned}
$$
Note we can taking $\pi$ s.t. $V$ and $V_n \circ \pi$ are independent when $n$ is fixed, then
$$
\mathop{{}\mathbb{E}}V_n \cdot V_n \circ \pi=(\mathop{{}\mathbb{E}} V_n)^2
$$
which in turn show that
$$
\begin{aligned}
    |\mathop{{}\mathbb{E}}V^2-(\mathop{{}\mathbb{E}}V_n)^2|&=|\mathop{{}\mathbb{E}}(V^2-V_n \cdot V_n \circ \pi)|
    \\ & \le 
    \mathop{{}\mathbb{E}}|V^2-V_n \cdot V_n\circ \pi|
    \\ & \le 
    2 \mathop{{}\mathbb{E}} |V-V_n|\to 0
\end{aligned}
$$
where the final step followed by noting:
$$
|V^2-V_n \cdot V_n \circ \pi|=|(V-V_n)V+(V-V_n \circ \pi)V_n|\le |V-V_n|+|V-V_n \circ \pi|
$$

:::


