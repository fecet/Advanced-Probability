## Sub-gaussian distributions in higher dimensions

Now we generalize sub-gaussian distributions to higher dimensions. Recall the characterization of multivariate normal distributions \@ref(exr:characterization-normal), it's natural to define 

::: {.definition  name="Sub-gaussian random vectors"}

A random vector $X$ in $\mathbb{R}^{n}$ is called sub-gaussian if the marginal $\bm{u'X}$ are sub-gaussian for all $\bm{u}\in \mathbb{R}^{n}$. The sub-gaussian norm is defined as
$$
\left\| X \right\|_{\psi_2}=\sup_{\bm{u} \in S^{n-1}} \left\| \bm{u'X} \right\|_{\psi_2}
$$

:::

Clearly, a random vector with independent, sub-gaussian coordinates is sub-gaussian by proposition \@ref(prp:sub-gaussian-sum), in such case, we have
$$
\left\| \bm{X} \right\|_{\psi_2}\le C \max_{i} \left\| X_{i} \right\|_{\psi_2}
$$
as $\bm{u}$ should be $(0,0,\dots,1,\dots,0)$.

If those coordinates are not independent, the $\left\| \cdot \right\|_{\psi_2}$ is not bounded.

::: {.exercise  name=""}

Let $\bm{X}\in \mathbb{R}^{n}$ be a random vector with sub-gaussian coordinates $X_i$, show that $X$ is a sub-gaussian. 

Nevertheless, find an example of a random vector $X$ with
$$
\left\| \bm{X} \right\|_{\psi_2}\gg \max_{i} \left\| X_i \right\|_{\psi_2}
$$

:::

::: {.proposition  name=""}

Let $\bm{X}\in \mathbb{R}^{n}$ be a $r.v.$ with sub-gaussian coordinates $X_{i}$, then $\bm{X}$ is a sub-gaussian $r.v.$.

:::

::: {.proof}

Note that $\left<x,\bm{X}\right>=\sum_{i=1}^{n} x_{i}X_{i}$ where $X_{i}$ are sub-gaussian distribution, so  $\left<x,\bm{X}\right>$ are sub-gaussian for every $x\in \mathbb{R}^{n}$ since the property of vector space.

:::

### Gaussian and Bernoulli distributions

Clearly, multivariate normal distribution $\mathcal{N}(\bm{\mu},\bm{\Sigma})$ is sub-gaussian, moreover, $\mathcal{N}(0,\bm{I})$ has sub-gaussian norm of order $O(1)$:
$$
\left\| \bm{X} \right\|_{\psi_2}\le C
$$
as all marginal $\bm{e'X}$ are $\mathcal{N}(0,1)$ as $\bm{e'e=1}$. Similarly, the multivariate symmetric Bernoulli distribution also has $O(1)$ norm.

### Discrete distributions

Recall the "worst" distribution, coordinate distribution, where $\bm{X}\sim \text{Unif}\left\{ \sqrt{n}\bm{e_i}:i=1,2,\dots,n \right\}$. It's sub-gaussian as it supported in a finite set, however, it has a larger norm:


::: {.proposition  name=""}

Show that for coordinate distribution $X$:
$$
\left\| X \right\|_{\psi_2}\asymp\sqrt{\frac{n}{\ln n}}
$$

:::


:::: {.proof}

Note $\left( \frac{nu_1^2}{t^2},\frac{nu_2^2}{t^2},\dots,\frac{nu_n^2}{t^2} \right)'$ is majored by $(\sum_{i=1}^{n} \frac{nu_i^2}{t^2},0,\dots,0)'$, Karamata's inequality yield
$$
\mathop{{}\mathbb{E}}_{}\exp \left( \frac{\bm{u'X}}{t^2} \right)=\frac{1}{n}\sum_{i=1}^{n} \left[ \exp \left( \frac{nu_i^2}{t^2} \right) \right]\le \frac{1}{n} \left[ \exp \left( n \sum_{i=1}^{n} \frac{u_i^2}{t^2} \right)+n-1 \right]
$$
Note $\left\| \bm{X} \right\|_{\psi_2}$ is just $\left\| \bm{e_i'X} \right\|_{\psi_2}$ for some $i$, and now the equity of Karamata's inequality holds. To find $\left\| \bm{X} \right\|_{\psi_2}$, we have
$$
\frac{1}{n}\left[ \mathop{{}\mathbb{E}}_{}\exp \left( \frac{n}{t^2} \right)+(n-1) \right] \le 2
$$
Substitute $t$ by $\left\| \bm{X} \right\|_{\psi_2}$,  solve it lead to $\left\| \bm{X} \right\|_{\psi_2}=\sqrt{\frac{n}{\ln(n+1)}} \asymp \sqrt{\frac{n}{\ln n}}$.



::::


More generally, discrete distributions do not make nice sub-gaussian distributions, unless they are supported on exponentially large sets:


::: {.proposition  name=""}

Let $\bm{X}$ be isotropic supported in a finite set $T\subset \mathbb{R}^{n}$. Show that if $\left\| X \right\|_{\psi_2}\ll 1$, then $\left| T \right|\ge e^{cn}$.

:::

### Uniform distribution on sphere

Good sub-gaussian random vectors not necessary have independent coordinates.


::: {.theorem #sphere-sub-gaussian name="Uniform distribution on the sphere is sub-gaussian"}

Suppose $\bm{X}\sim \text{Unif}(\sqrt{n} S^{n-1})$, then $\bm{X}$ is sub-gaussian and $\left\| X \right\|_{\psi_2} \le C$.

:::


:::: {.proof}

As proposition \@ref(prp:normal-sphere), we can represent $X$ as
$$
\bm{X}=\sqrt{n} \frac{\bm{g}}{\left\| \bm{g} \right\|_{2}}
$$
where $\bm{g}\sim \mathcal{N}(\bm{0,I})$. It's sufficient to show that $\bm{\theta'X}$ is sub-gaussian for all $\bm{\theta}$ and WLOG we may assume $\bm{\theta}=(1,0,\dots,0)$. In which case, we want to bounded
$$
p(t):=\mathop{{}\mathbb{P}}\left\{ \left| X_1 \right|\ge t \right\}=\mathop{{}\mathbb{P}}\left\{ \frac{\left| g_1 \right|}{\left\| \bm{g} \right\|_{2}}\ge  \frac{t}{\sqrt{n}} \right\}
$$
As $\left\| \bm{g} \right\|_2-\sqrt{n}$ is sub-gaussian with order $O(1)$ by theorem \@ref(thm:norm-concentration), we reduce this to bounding $\mathop{{}\mathbb{P}}\left\{ \left| g_1 \right|\ge t \right\}$, and it's clearly sub-gaussian again.

Explicitly, take $\varepsilon:=\left\{ \left\| \bm{g} \right\|_{2}\ge \frac{\sqrt{n}}{2} \right\}$, then
$$
\begin{aligned}
    \mathop{{}\mathbb{P}}\left\{ \varepsilon^c \right\}&=
    \mathop{{}\mathbb{P}}\left\{ \left\| \bm{g} \right\|_{2}< \frac{\sqrt{n}}{2} \right\}=\mathop{{}\mathbb{P}}\left\{ \left\| \bm{g} \right\|_{2}-\sqrt{n}< -\frac{\sqrt{n}}{2} \right\} 
    \\&\le \mathop{{}\mathbb{P}}\left\{ \left| \left\| \bm{g} \right\|_{2}-\sqrt{n} \right|>\frac{\sqrt{n}}{2} \right\}\le 2 \exp \left( -cn \right)
\end{aligned}
$$
It follows that
$$
\begin{aligned}
    p(t)&\le \mathop{{}\mathbb{P}}\left\{ \frac{\left| g_1 \right|}{\left\| \bm{g} \right\|_{2}}\ge  \frac{t}{\sqrt{n}} \cap \varepsilon \right\}+\mathop{{}\mathbb{P}}\varepsilon^c
    \\&\le 
    \mathop{{}\mathbb{P}}\left\{ \left| g_1 \right|\ge \frac{t}{2}\right\}+2\exp(-cn)
    \\& \le 2\exp(-t^2 / 8)+2\exp(-cn)
\end{aligned}
$$
Note when $\left| X_1 \right|\le \left\| \bm{X} \right\|_{2}=\sqrt{n}$ and that avoid $t<\sqrt{n}$. For $t\le \sqrt{n}$, we clearly have $p(t)\le C\exp(-c't^2)$ as desired. The second statement follows by all marginal distribution of sphere are identical.

::::

Furthermore, we can extend sphere to a ball then claim remains true: 

::: {.proposition  name="Uniform distribution on the Euclidean ball"}

Suppose $\bm{Y}\sim \text{Unif}(B(0,\sqrt{n}))$, then it's sub-gaussian with $O(1)$ norm.

:::


:::: {.proof}

Let $r:= \frac{\left\| \bm{Y} \right\|_2}{\sqrt{n}}\le 1$ and $\bm{X:=\frac{Y}{r}}$, then $\bm{X}\sim \mathop{\text{Unif}}\left( \sqrt{n}S^{n-1} \right)$. By theorem \@ref(thm:sphere-sub-gaussian), we have $\left\| \bm{X} \right\|_{\psi_2}\le C$. Note
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}_{}\exp \left( \frac{\langle \bm{Y},\bm{\theta} \rangle^2 }{t^2} \right)&=\mathop{{}\mathbb{E}}_{}\exp \left( \frac{r^2\langle \bm{X},\bm{\theta} \rangle^2 }{t^2} \right)\le \mathop{{}\mathbb{E}}_{}\exp \left( \frac{\langle \bm{X},\bm{\theta} \rangle^2 }{t^2} \right)
\end{aligned}
$$
thus $\left\| \bm{Y} \right\|_{\psi_2}\le \left\| \bm{X} \right\|_{\psi_2}\le C$.


::::




::: {.theorem  name="Projective limit theorem"}

If $X\sim \text{Unif}(\sqrt{n} S^{n-1})$, then for any $\bm{\theta}\in \mathbb{R}^{n}$, as $n\to \infty$,
$$
\bm{\theta'X}\xrightarrow{d} \mathcal{N}(0,1)
$$


:::

Thus we can view theorem \@ref(thm:sphere-sub-gaussian) as a concentration version of the Projective Limit Theorem, in the same sense as Hoeffdingâ€™s inequality is a concentration version of the classical CLT.

### Uniform distribution on convex sets

Related to [Isotropic convex sets], we consider uniform distribution on a convex body $K$, as we seen, it can be an isotropy. Now is $\bm{X}$ always sub-gaussian?


::: {.proposition  name=""}

Suppose a $\ell_1$ norm ball in $\mathbb{R}^{n}$ and $\bm{X_{n}} \sim \mathop{\text{Unif}}\left( K \right)$.
$$
K:= \left\{ \bm{x} \in \mathbb{R}^{n}: \left\| \bm{x} \right\|_1 \le r \right\}
$$

1.  $\bm{X}$ is isotropic for some $r \asymp n$.
2.  $\left\| \bm{X} \right\|_{\psi_2}$ can not bounded by some $C$ as $n$ grows.


:::

::: {.proof}

1.
    Let $\bm{X}\sim \mathop{\text{Unif}}(K)$, then it is clear that
    $$
    \mathop{{}\mathbb{E}}X_{i}=0 \text{ and }\mathop{{}\mathbb{E}}X_{i}X_j=0 \text{ for }i\neq j
    $$
    for the sphere symmetry and to prove it is isotropic, just need to show that $\mathop{{}\mathbb{E}}X_{i}^{2}=1$ for each $i$.  
    Note that
    $$
    \mathop{{}\mathbb{E}}X_{i}^{2}=\int_0^{\infty}\mathop{{}\mathbb{P}}\left\{X_{i}^{2} \ge t\right\}dt=\int_0^{\infty}\mathop{{}\mathbb{P}}\left\{\left|X_{i}\right| \ge x  \right\}\cdot 2xdx
    $$
    Consider that
    $$
    \mathop{{}\mathbb{P}}\left\{\left|X_{i}\right| \ge x\right\}=\frac{\left(r-x\right)^{n}}{r^{n}}
    $$
    since the volume of the ball, and the probability vanishes when $x\gt r$
    $$
    \mathop{{}\mathbb{E}}X_{i}^{2}=\int_0^{r}2x\cdot \frac{\left(r-x\right)^{n}}{r^{n}}dx=\frac{2r^{2}}{n^{2}+3n+2}
    $$
    when $\mathop{{}\mathbb{E}}X_{i}^{2}=1$, we have
    $$
    r=\sqrt{\frac{n^{2}+3n+2}{2}}=\Omega(n)
    $$

2.
    Consider the $L^p_{}$ norm of $X_{i}$.
    $$
    \left\|X_{i}\right\|_{L^p_{}}^{p}=\int_0^{\infty}px^{p-1}\mathop{{}\mathbb{P}}\left\{\left|X_{i}\right|\gt x\right\}dx=pr^{p}\frac{\Gamma(p)\Gamma(n+1)}{\Gamma(n+p+1)}
    $$
    so when $n\to \infty$ and $p\to \infty$, we have
    $$
    \left\|X_i\right\|_{L^p_{}}=\frac{r}{n}\cdot \Gamma(p)^{1/p}=\Omega(p)
    $$
    so there does not exist a constant  $C$ $s.t.$ $\left\|X_{i}\right\|_{L^{p}}\le C\sqrt{p}$ for any $n$.

:::


























<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
