# Preliminaries on random variables

::: {.lemma  name=""}

Let $X$ be a non-negative r.v. then
$$
\mathbb{E}X=\int_0^{\infty}\mathbb{P}\left\{X\gt t\right\}dt
$$

:::

::: {.proof}

Note that for Lebesgue measure, there exists:
$$
x=\mu [0,x)=\mu(0,x)=\mu[0,x]=\int_0^{\infty}\bm{\mathbf{1}}_{\left\{t\lt x\right\}}dt
$$
Then for some  certain $\omega\in \Omega$, 
$$
X(\omega)=\int_0^{\infty}\bm{\mathbf{1}}_{t\lt X(\omega)}dt
$$
Then 
$$
\mathbb{E}X=\int_\Omega Xd\mathbb{P}=\int_\Omega \int_0^{\infty}\bm{\mathbf{1}}_{\left\{t\lt X\right\}}dt d\mathbb{P}
$$
According to Fubini's Theorem,
$$
\mathbb{E}X=\int_0^{\infty}dt\int_\Omega \bm{\mathbf{1}}_{\left\{t\lt X\right\}}d\mathbb{P} =\int_0^{\infty}\mathbb{P}\left\{X\gt t\right\}dt
$$

:::


::: {.exercise  name="Generalization of integral identity"}

Show that for any $r.v.$ $X$
$$
\mathop{{}\mathbb{E}}_{}X=\int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X>t\}dt-\int_{-\infty}^{0}\mathop{{}\mathbb{P}}\{X<t\}dt
$$

:::

::: {.solution}

Note $X=X^{+}-X^{-}$, then
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}_{}X&=\mathop{{}\mathbb{E}}_{}X^{+}-\mathop{{}\mathbb{E}}_{}X^{-}
    \\ &= 
    \int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X^{+}>t\}dt-\int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X^{-}>t\}dt
    \\ &= 
    \int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X>t\}dt-\int_{0}^{\infty} \mathop{{}\mathbb{P}}\{-X>t\}dt
    \\ &= 
    \int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X>t\}dt-\int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X<-t\}dt
    \\ &= 
    \int_{0}^{\infty} \mathop{{}\mathbb{P}}\{X>t\}dt-\int_{-\infty}^{0} \mathop{{}\mathbb{P}}\{X<t\}dt
\end{aligned}
$$
:::


::: {.exercise  name="$p$-moments via tails"}

Let $X$ be a $r.v.$ and $p\in (0,\infty)$, show that
$$
\mathop{{}\mathbb{E}}_{} \left| X \right|^{p}=\int _{0}^{\infty} pt ^{p-1} \mathop{{}\mathbb{P}}\{\left| X \right|>t\}dt
$$

:::


::: {.solution}

By the integral identity
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}_{}\left| X \right|^{p}&=\int _{0}^{\infty} \mathop{{}\mathbb{P}}\{\left| X \right|^{p}>t\}dt
    \\ &= 
    \int _{0}^{\infty} \mathop{{}\mathbb{P}}\{\left| X \right|>t^{\frac{1}{p}}\}dt
    \\ &= 
    \int _{0}^{\infty} \mathop{{}\mathbb{P}}\{\left| X \right|>t^{\frac{1}{p}}\}\frac{p d(t ^{\frac{1}{p}})}{t ^{\frac{1}{p}-1}}
    \\ &= 
    \int _{0}^{\infty} \mathop{{}\mathbb{P}}\{\left| X \right|>t\}\frac{p dt}{t ^{1-p}}
    \\ &= \int _{0}^{\infty} pt ^{p-1} \mathop{{}\mathbb{P}}\{\left| X \right|>t\}dt
\end{aligned}
$$

:::

::: {.exercise #chebyshev-inequality  name="Chebyshev's inequality"}

Let $X$ be a $r.v.$ with mean $\mu$ and variance $\sigma^2$, then for any $t>0$, we have
$$
\mathop{{}\mathbb{P}}\{\left| X-\mu \right|\ge t\}=
\mathop{{}\mathbb{P}}\{\left| X-\mu \right|^2\ge t^2\}=\frac{\sigma^2}{t^2}
$$

:::

::: {.theorem #demoivre-laplace name="DeMoivre-Laplace Theorem"}

Let $(X_{i})_{i \in \mathbb{N}^*}$ are $i.i.d.$ Bernoulli variables with mean $\mu=p$ and variance $\sigma^2=p(1-p)$, then
$$
\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\xrightarrow{d} \mathfrak{Z}
$$

:::


::: {.theorem #CLT name="Lindeberg-Levy Theorem"}

Let $(X_{i})_{i \in \mathbb{N}^*}$ be $i.i.d.$ with mean $\mu$ and variance $\sigma^2$, both finite, then
$$
Z_n=\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\xrightarrow{d} \mathfrak{Z}
$$

:::


::: {.proof}

In view of theorem \@ref(thm:levy-continuity), the claim is
$$
\varphi_{Z_n}(t)\to \varphi_{\mathfrak{Z}}(t)=e^{-t^2 / 2}
$$
Let $\varphi$ denote the $ch.f.$ of $\frac{X_n-\mu}{\sigma}$, then Taylor's theorem yields
$$
\begin{aligned}
    \varphi(t)&=\varphi(0)+\varphi'(0)t+\frac{1}{2}\varphi''(0)t^2(1+h(t))
    \\ &= 
    1-\frac{1}{2}t^2(1+h(t))
\end{aligned}
$$
for some $h$ $s.t.$ $\lim_{t \to \infty}\left|h(t)\right|=0$. As $(X_n)$ are independent, note $Z_n=\sum_{}^{}\frac{X_n-\mu}{\sigma} / \sqrt{n}$:
$$
\varphi_{Z_n}(t)=\varphi^{n}(\frac{t}{\sqrt{n}})=[1-\frac{r^2 / 2}{n}(1+h(\frac{r}{\sqrt{n}}))]^{n}
$$
note $\frac{r}{\sqrt{n}}\to 0$ as $n \to \infty$, thus
$$
[1-\frac{r^2 / 2}{n}(1+h(\frac{r}{\sqrt{n}}))]^{n}\to (1-\frac{r^2 / 2}{n})^{n}\to e^{-r^2 / 2}
$$
and claim follows.

:::


::: {.exercise  name=""}

Let $(X_{i})_{i \in \mathbb{N}^*}$ be $i.i.d.$ $r.v.'s$ with mean $\mu$ and finite variance, then
$$
\mathop{{}\mathbb{E}}_{}\left| \frac{1}{N} \sum_{i=1}^{N} X_i -\mu \right|=O(\frac{1}{\sqrt{N}})
$$

:::


::: {.solution}

Suppose the variance is $\sigma^2<\infty$, then 
$$
\begin{aligned}
    \lim_{N \to \infty}\mathop{{}\mathbb{E}}_{}\left| \frac{1}{N} \sum_{i=1}^{N} X_i -\mu \right|&=\lim_{N \to \infty}  \mathop{{}\mathbb{E}}_{}\left| \frac{1}{N} \sum_{i=1}^{N} (X_i-\mu) \right|
    \\ &= \mathop{{}\mathbb{E}}_{}\lim_{N \to \infty} \left| \frac{S_n-n\mu}{N} \right| 
    \\ &= 
    \frac{\sigma}{\sqrt{N}} \mathop{{}\mathbb{E}}_{}\lim_{N \to \infty} \left| \frac{S_n-n\mu}{\sigma \sqrt{N}}\right| 
    \\ &= 
    \frac{\sigma}{\sqrt{N}} \mathop{{}\mathbb{E}}_{} \mathfrak{\left| Z \right|}=\frac{\sigma\sqrt{2}}{\sqrt{N\pi}}=O(\frac{1}{\sqrt{N}})
\end{aligned}
$$

:::
