## General Hoeffding's and Khintchine's inequalities

Recall that a sum of independent normal $r.v.$ $X_i$ is normal, that is a form of the **rotation invariance** of normal distribution.  
The rotation invariance property extends to general sub-gaussian distributions,
albeit up to an absolute constant.

::: {.proposition #sub-gaussian-sum name="Sums of independent sub-gaussians"}

Let $X_{i},i=1,2,\ldots,n$ be independent, mean zero sub-gaussian r.v. Then $\sum_{i=1}^{n} X_{i}$ is also sub-gaussian and
$$
\left|\left|\sum_{i=1}^{n} X_{i}\right|\right|_{\psi_2}^{2}\le C \sum_{i=1}^{n} \|X_{i}\|_{\psi_2}^{2}
$$
where $C$ is an absolute constant.

:::

::: {.proof}

Consider the MGF of the sum $\sum_{i=1}^{n} X_{i}$.
$$
\begin{aligned}
    \mathbb{E}\exp\left(\lambda \sum_{i=1}^{n} X_{i}\right)&= \prod_{i=1}^{n} \mathbb{E}\exp(\lambda X_{i}) \\
    &\le \prod_{i=1}^{n} \exp(C\lambda^{2}\|X_{i}\|_{\psi_2}^{2})  \\
    &= \exp(\lambda^{2}K^{2})
\end{aligned}
$$
where $K^{2}=C \sum_{i=1}^{n} \|X_{i}\|_{\psi_2}^{2}$. For a sub-gaussian r.v. $X=\sum_{i=1}^{n} X_{i}$ with $\mu_X=0$, note that
$$
\mathbb{E}\exp(\lambda X)\le \exp(\lambda^{2}K^{2})
$$
implies that there exists $c\in \mathbb{R}$ s.t. 
$$
\mathbb{E}\exp\left(\frac{X^{2}}{(cK)^{2}}\right)\le 2
$$
Then recall the definition of $\|\cdot \|_{\psi_2}$ we find that there exists $c\in \mathbb{R}$ s.t. 
$$
\|X\|_{\psi_2}\le c\sqrt{\sum_{i=1}^{n} \|X_{i}\|_{\psi_2}^{2}}
$$
:::

::: {.theorem #general-hoeffding-sub-gaussian name="General Hoeffding inequality"}

Let $X_1,\ldots,X_n$ be independent, mean-zero, sub-gaussian r.v. and $a=(a_1,\ldots,a_n)\in \mathbb{R}^{n}$. Then for every $t\ge 0$, we have
$$
\mathbb{P}\left\{\left|\sum_{i=1}^{n} a_{i}X_{i}\right|\ge t\right\}\le 2\exp\left(-\frac{ct^{2}}{K^{2}\|a\|_{2}^{2}}\right)
$$
where $K=\max_{i}\|X_{i}\|_{\psi_2}$

:::

::: {.proof}

Let $X=\sum_{i=1}^{n} X_{i}$ with $\mu_X=0$, sub-gaussian r.v. 
$$
\begin{aligned}
    \mathbb{P}\left\{\sum_{i=1}^{n} a_{i}X_{i}\ge t\right\}&= \mathbb{P}\left\{\exp\left(\lambda\sum_{i=1}^{n} a_{i}X_{i}\right)\ge \exp(\lambda t)\right\} \\
    &\le \exp(-\lambda t)\cdot \mathbb{E}\exp\left(\lambda \sum_{i=1}^{n} a_{i}X_{i}\right) \\
    &= \exp(-\lambda t)\cdot \prod_{i=1}^{n} \mathbb{E}\exp(\lambda a_{i}X_{i}) \\
\end{aligned}
$$
Note that $X_{i}$ are sub-gaussian r.v. 
$$
\mathbb{E}\exp(\lambda a_{i}X_{i})\le \exp(K_{i}^{2}(a_{i}\lambda)^{2})
$$
then
$$
\prod_{i=1}^{n} \mathbb{E}\exp(\lambda a_{i}X_{i})\le \exp\left(\lambda^{2}\sum_{i=1}^{n} K_{i}^{2}a_{i}^{2}\right)\le \exp\left(cK^{2}\lambda^{2}\sum_{i=1}^{n} a_{i}^{2}\right)=\exp(cK^{2}\lambda^{2}\|a\|_{2}^{2})
$$
where $K=\max_{i}K_{i},K_{i}=\sqrt{c}\|X_{i}\|_{\psi_2}$, then
$$
\mathbb{P}\left\{\sum_{i=1}^{n} a_{i}X_{i}\ge t\right\}\le \exp(-\lambda t)\cdot \exp(cK^{2}\lambda^{2}\|a\|_{2}^{2})\text{ for all }\lambda \gt 0
$$
then we have
$$
\begin{aligned}
    \mathbb{P}\left\{\sum_{i=1}^{n} a_{i}X_{i}\ge t\right\}&\le \exp(-\lambda t+\alpha \lambda^{2})\text{ where }\alpha=cK^{2}\|a\|_{2}^{2} \\
    \mathbb{P}\left\{\sum_{i=1}^{n} a_{i}X_{i}\ge t\right\}&\le \exp\left(-\frac{t^{2}}{4\alpha}\right) \text{ by letting } \lambda=\frac{t}{2\alpha}\\
\end{aligned}
$$
so
$$
\mathbb{P}\left\{\sum_{i=1}^{n} a_{i}X_{i}\ge t\right\}\le \exp\left(-\frac{ct^{2}}{K^{2}\|a\|_{2}^{2}}\right)\text{ where }K=\max_{i}\|X_{i}\|_{\psi_2}
$$

:::


::: {.exercise  name=""}

Deduce Hoeffding's inequality for bounded $r.v.$ \@ref(thm:general-hoeffding) from theorem \@ref(thm:general-hoeffding-sub-gaussian)

:::

::: {.solution}

Note for bounded $r.v.$, we have $\left\| X_i-\mu \right\|_{\psi_2}\le \frac{1}{\ln 2} (M_i-\mu_i)\le \frac{1}{\ln 2}(M_i-m_i)$, then
$$
\mathop{{}\mathbb{P}}\left\{ \sum_{i=1}^{n} (X_i-\mu_i)>t \right\} \le \exp \left( - \frac{c t^2 (\ln 2)^2}{(M_i-m_i)^2} \right)
$$

:::



::: {.theorem  name="Khintchine's inequality"}

Let $X_1,\ldots,X_n$ be independent sub-gaussian r.v. with zero-mean and unit variances, and let $a=(a_1,\ldots,a_n)\in \mathbb{R}^{n}$, then for any $p \in [2,\infty)$, we have
$$
\left\| a \right\|_{2}\le \left\| \sum_{i=1}^{n} a_{i}X_{i} \right\|_{p}\le CK\sqrt{p} \left\| a \right\|_{2}
$$
where $K=\max_i \left\| X_i \right\|_{\psi_2}$ and $C$ is an absolute constant.

And for $p \in (0,2)$, we have
$$
c(K) \left\| a \right\|_2 \le \left\| \sum_{i=1}^{n}a_iX_i \right\|_p \le \left\| a \right\|_2
$$
where $c(K)$ is some function of $K$.

:::

:::: {.proof}

**Step 1**. We first show that for $p \in [2,\infty)$. Note $\mathop{\text{Var}}X_i=1$, the first inequality is just consequence of $\left\| \cdot \right\|_{2} \le \left\| \cdot \right\|_{p}$. For another, by Hoeffding's inequality \@ref(thm:general-hoeffding-sub-gaussian)
$$
\begin{aligned}
    \mathop{{}\mathbb{E}}_{} \left| \sum_{i=1}^{n}a_iX_i \right|^{p}&= \int_{0}^{\infty} pt ^{p-1} \mathop{{}\mathbb{P}}\left\{ \left| \sum_{i=1}^{n}a_iX_i \right|>t \right\}dt
    \\ &\le 
    2p \int_{0}^{\infty} t ^{p-1} \exp \left( - \frac{ct^2}{K^2 \left\| a \right\|_2^2} \right)dt
    \\ &= 
    p \left( \frac{K \left\| a \right\|_2}{\sqrt{c}} \right)^{p} \Gamma \left( \frac{p}{2} \right)
\end{aligned}
$$
Taking $p$th root and claim follows by Stirling's formula
$$
\left[ {p \Gamma \left( \frac{p}{2} \right)} \right]^{\frac{1}{p}} = O(p^{\frac{1}{p}} \left( \frac{p}{2} \right)^{\frac{p-1}{2}})=O(\sqrt{p})
$$

**Step 2** Then we show that for $p \in (0,2)$. By Cauchy-Schwartz inequality (where $\bm{u}=\mathop{{}\mathbb{E}}_{}\left| \sum_{}^{} \right|^{\frac{p}{2}}$ and $\bm{v}=\mathop{{}\mathbb{E}}_{}\left| \sum_{}^{} \right|^{2-\frac{p}{2}}$):
$$
\mathop{{}\mathbb{E}}_{}\left| \sum_{i=1}^{n} a_i X_i \right|^{p} \ge \left( \mathop{{}\mathbb{E}}_{}\left| \sum_{i=1}^{n} a_i X_i \right|^{2} \right)^{2} \bigg/  \mathop{{}\mathbb{E}}_{} \left| \sum_{i=1}^{n}a_iX_i \right|^{4-p}
$$
Note $4-p \ge  2$ and we have
$$
\begin{aligned}
    \text{LHS}& \ge \frac{\left\| a \right\|_2^{4}}{\left( CK \sqrt{4-p} \left\| a \right\|_2 \right)^{4-p}}= c(K)^{p} \left\| a \right\|_2^{p}
\end{aligned}
$$
where $c(K)=\left( CK \sqrt{4-p} \right)^{1-\frac{4}{p}}$.


::::


### Centering

In results like Hoeffding's inequality, we typically assume $X_i$ has zero mean. We can do so as the centering $X_i-\mu_i$ inherit most of properties of $X_i$. For example, $X_i-\mu_i$ keep in the $L^2$ space.

::: {.proposition  name="$L^{2}$ bound"}

$$
\|X-\mathbb{E}X\|_{{2}}\le \|X\|_{{2}}
$$

:::

::: {.proof}

Suppose $X$ is bounded. Note that
$$
\int(X-\mu)^{2}=\int(X^{2}-2\mu X+\mu^{2})=\mathbb{E}X^{2}-\mu^{2}\le \mathbb{E}X^{2}
$$

:::

And there is a similar results for sub-gaussian norm.

::: {.proposition  name="centering"}

If $X$ is sub-gaussian, then $X-\mathbb{E}X$ is sub-gaussian and
$$
\|X-\mathbb{E}X\|_{\psi_2}\le C\|X\|_{\psi_2}
$$
where $C$ is absolute constant.

:::

::: {.proof}

Triangle inequality yield:
$$
\|X-\mathbb{E}X\|_{\psi_2}\le \|X\|_{\psi_2}+\|\mathbb{E}X\|_{\psi_2} 
$$

:::


::: {.exercise  name=""}

Show that the centering inequality above does not hold with $C=1$. 

:::

:::: {.solution}

Construct $r.v.$ $s.t.$:
$$
X=\begin{cases}
    a& \text{with probability } p
    \\
    -a& \text{with probability } 1-p
\end{cases}
$$
where $a=\sqrt{\ln 2}$, it's clear that
$$
\mathop{{}\mathbb{E}}_{}\exp(X^2)=2 \implies \left\| X \right\|_{\psi_2}=1
$$
Then we claim that $\left\| X-\mu \right\|_{\psi_2}> \left\| X \right\|_{\psi_2}$. Note $\mu=(2p-1)a$, we have
$$
X-\mu=\begin{cases}
    2(1-p)a& \text{with probability } p
    \\
    -2pa& \text{with probability } 1-p
\end{cases}
$$
and thus
$$
\mathop{{}\mathbb{E}}_{} \exp \left( \left| X-\mu \right|^2 \right)=p2^{4(1-p)^2}+(1-p)2^{4p^2}
$$
which cannot bounded by $2$ certainly.

```{r centering-psi, fig.align = "center", fig.cap="MGF of centering square", out.width = '70%'}

knitr::include_graphics("figures/centering1.pdf")

```

::::















