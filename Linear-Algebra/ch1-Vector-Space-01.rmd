
## Linear independence and basis

::: {.definition  name="linear independence"}

A family of vectors $\left\{ x_{i} \right\}_{i\in I}$ is called **linear independent** if the vectors $x_{i}$ are linearly independent i.e. 
$$ 
\sum_{i\in I}^{} \alpha_{i}x_{i}=0\implies \alpha_{i}=0\ \mathop{\text{ for each }}i
$$

:::

::: {.definition  name="system of generators"}

A subset $S\subset E$ is called a system of generators of $E$ if every vector $x\in E$ is a linear combination of vectors in $S$.

:::

::: {.proposition  name=""}

1.  Every finitely generated non-trivial vector space has a finite basis.
2.  Suppose that $S=\left\{ x_1,\ldots,x_m \right\}$ is a finite system of generators of $E$ and that the subset $R\subset S$ by $R=\left\{ x_1,\ldots,x_r \right\}\left(r\le m\right)$ consists of linearly independent vectors. Then there exists a basis $T$ of $E$ s.t. $R\subset T\subset S$.

:::

::: {.proof}

Just need to notice that every basis is the system of generators, and it is a minimal one.

:::

::: {.theorem  name=""}

Let $E$ be a non-trivial vector space. Suppose $S$ is a system of generators and $R$ is a family of linearly independent vectors in $E$ s.t. $R\subset S$. Then there exists a basis $T$ of $E$ s.t. $R\subset T\subset S$.

:::

::: {.proof}

Consider the partially order defined between $R$ and $S$, find some $X\subset E$ s.t. 

-   $R\subset X\subset S$ 
-   the vectors in $X$ are linearly independent.

We note this partially order as $\mathcal{P}\left(R,S\right)$.  
Notice that for every chain $\left\{ X_\alpha \right\}\subset \mathcal{P}\left(R,S\right)$ has a maximal element $A=\bigcup_{\alpha} X_\alpha$. It is obvious that $A\in  \mathcal{P}\left(R,S\right)$ (Notice that $R\subset A\subset S$ and the property of a chain of the set that contains linearly independent vectors.)  
So we prove that every chain $\left\{ X_\alpha \right\}\subset \mathcal{P}\left(R,S\right)$ has a upper bound in $\mathcal{P}\left(R,S\right)$, so Zorn's Lemma implies that there exists a maximal element $T\in \mathcal{P}\left(R,S\right)$ s.t. vectors in $T$ are linearly independent.  
Then we just need to show that $T$ generates $E$. Give $x\in E$, suppose that $x$ is linearly independent to vectors in $T$. Notice that  $S$ generates $E$, so 
$$ 
x=\sum_{i\in I'}^{} \alpha_i x_{i} \qquad \mathop{\text{for some }}x_{i}\in S
$$
If $x$ is linearly independent to vectors in $T$ then exists some $i\in I'$ s.t. $x_{i}$ is linearly independent to vectors in $T$ and note this set as $\left\{ x_{j} \right\}_{j\in J}\subset S$, consider the set $\left\{ x_{j} \right\}_{j\in J}\cup T\supsetneqq T$ which leads to a contradiction of the maxmality of $T$. So $T$ is a basis of $E$.

:::

::: {.corollary  name=""}

1.  Every system of generators of $E$ contains a basis. In particular, every non-trivial vector space has a basis.
2.  Every family of linearly independent vectors of $E$ can be extended to a basis.

:::

## Free vector space

Let $X$ be an arbitrary set and consider all maps $f:X\to \mathbb{K}$ s.t. $f\left(x\right)\neq 0$ only for finitely many $x\in X$, denoting the set of these maps by $F\left(X\right)$, it is easy to show that $F\left(X\right)$ is a vector space.

Now give a basis of $F\left(X\right)$. For any $a\in X$, let $f_a$ be:
$$ 
f_a\left(x\right)=
\begin{cases}
    1 &x=a \\
    0 &x\neq a
\end{cases}
$$
Then $\left\{ f_a \right\}_{a\in X}$ forms a basis of $F\left(X\right)$.

$F\left(X\right)$ is called the **free vector space over $X$**.

## Linear mappings


::: {.definition  name="linear mapping"}

Suppose that $E$ and $F$ are vector spaces, and let $\phi:E\to F$ be a set mapping s.t. 
$$ 
\phi\left(x+y\right)=\phi\left(x\right)+\phi\left(y\right) \mathop{\text{ for all }}x,y\in E
$$
and 
$$ 
\phi\left(\alpha x\right)=\alpha\phi\left(x\right) \mathop{\text{ for all }}\alpha\in \mathbb{K}, x\in E
$$
Then we call the mapping $\phi$ satisfying above conditions linear mappings.  
Moreover, if $F=\mathbb{K}$, then we called $\phi$ a **linear function** on $E$.

:::

::: {.corollary  name=""}

Linear mappings preserve linear relations.

:::

::: {.proof}

Suppose $\phi$ be a linear mappings, and let $u=\alpha x+\beta y\in E$, then
$$ 
\phi\left(u\right)=\phi\left(\alpha x+\beta y\right)=\alpha \phi\left(x\right)+\beta\phi\left(y\right)
$$

:::

Let $\phi:E\to F, \psi:F\to G$ be linear mappings, then the composition of them $\psi \circ \phi:E\to G$ is defined by:
$$ 
\left(\psi \circ \phi\right)\left(x\right)=\psi \left(\phi\left(x\right)\right)
$$
It is easy to show that $\psi \circ \phi$ is still a linear mapping.


::: {.proposition  name=""}

Suppose $S$ is a system of generators of $E$ and $\phi_0:S\to F$ where $F$ is also a vector space. Then $\phi_0$ can be extended in at most one way to linear mapping $\phi:E\to F$.  
And the extension exists iff such an extension is that 
$$ 
\sum_{i}^{} \alpha_{i}\phi_0\left(x_{i}\right)=0
$$
whenever $\sum_{i}^{} \alpha_{i}x_{i}=0$.

:::

::: {.proof}

-
    $\implies$ : Suppose $\phi$ to be a linear mapping and it is the extension of $\phi_0$, then $\phi\left(\sum_{i=1}^{n} \alpha_{i}x_{i}\right)=\sum_{i=1}^{n} \alpha_{i}\phi\left(x_{i}\right)$ for each $x_{i}\in E$.  
    And for each $x_{i}\in S$, 
    $$ 
    \phi\left(\sum_{i=1}^{n} \alpha_{i}x_{i}\right)=\sum_{i=1}^{n} \alpha_{i}\phi\left(x_{i}\right)=\sum_{i=1}^{n} \alpha_{i}\phi_0\left(x_{i}\right)
    $$
    so $\phi\left(0\right)=\phi_0\left(0\right)=0$.

-
    $\Longleftarrow$ : For any $x\in E$, define there exists some $\left\{ x_{i} \right\}_{i\in I}\subset S$ s.t. $x=\sum_{i\in I}^{} \alpha_{i}x_{i}$. Define
    $$ 
    \phi\left(x\right)=\sum_{i\in I}^{} \alpha_{i}\phi_0\left(x_{i}\right)
    $$
    It is obvious that $\phi$ is that linear mapping.

:::

Notice that if $S$ is a basis of $E$, let $\phi_0$ be a set map from $S$ to $E$, then $\phi_0$ can be extended in a unique way to a linear mapping $\phi:E\to F$.

::: {.proposition  name=""}

Let $\phi:E\to F$ be a linear mapping and $\left\{ x_\alpha \right\}$ be a basis of $E$. Then $\phi$ is a linear isomorphism iff the vectors $y_\alpha=\phi\left(x_\alpha\right)$ form a basis for $F$.

:::

::: {.proof}

-   
    $\implies$:
    As $\phi$ is a linear isomorphism, so for any $y\in F$, there exists a unique $x\in E$ s.t. $x=\phi^{-1}\left(y\right)$. Notice that $\left\{ x_\alpha \right\}$ is a basis, so $x=\sum_{\alpha}^{} a_{\alpha}x_{\alpha}$ for some $a_\alpha$, so $y=\phi\left(x\right)=\phi\left(\sum_{\alpha}^{} a_\alpha x_\alpha\right)=\sum_{\alpha}^{} a_\alpha\phi\left(x_\alpha\right)$. That means $\left\{ \phi\left(x_\alpha\right) \right\}$ generates $F$. Then we need to prove the linear independence.  
    Let $\sum_{\alpha}^{} \lambda_\alpha x_\alpha=0$, then $\lambda_\alpha=0$ for each $\alpha$. Then let $\sum_{\alpha}^{} \gamma_\alpha\phi\left(x_\alpha\right)=0$, then 
    $$ 
    \sum_{\alpha}^{} \gamma_\alpha\phi\left(x_\alpha\right)=\phi\left(\sum_{\alpha}^{} \gamma_\alpha x_\alpha\right)=0
    $$
    so $\sum_{\alpha}^{} \gamma_\alpha x_\alpha=0$ which means $\gamma_\alpha=0$ for each $\alpha$. So $\left\{ \phi\left(x_\alpha\right) \right\}$ is a basis of $F$.

-
    $\Longleftarrow$ :
    Let $\left\{ y_\alpha=\phi\left(x_\alpha\right) \right\}$ be a basis of $F$, then for each $y\in F$, there exists a unique components $\left(\lambda_\alpha\right)$ s.t. $\sum_{\alpha}^{} \lambda_\alpha y_\alpha=y$. Then we have
    $$ 
    \sum_{\alpha}^{} \lambda_\alpha \phi\left(x_\alpha\right)=\phi\left(\sum_{\alpha}^{} \lambda_\alpha x_\alpha\right)=\phi\left(x\right)
    $$
    for some unique $x\in E$.
    
:::

## Subspace and factor space


::: {.definition  name="Subspace"}

Let $X$ be a vector space and let $A\subset X$ be a subset of $X$. Then $A$ is called a subspace if $A$ is also a vector space.

Let $S$ be a non-empty subset of $X$ and there exists a set, noting as $X_S$, is the linear combination of any vectors in $S$, $X_S$ is truly a subspace which is called **the subspace generated by $S$** or **linear closure** of $S$.

:::

::: {.proposition  name=""}

Let $A_1,A_2$ be two subspaces of the vector space $X$ and suppose that $A_1\cap A_2\neq \varnothing$ then $A_1\cap A_2$ is still a subspace of $X$.

:::

::: {.definition  name="sum of subspace"}

Let $A_1,A_2$ be two subspaces of a vector space $X$, then $\left\{ x=x_1+x_2:x_1\in A_1,x_2\in A_2 \right\}$ is called the **sum of $A_1$ and $A_2$**, denote as $A_1+ A_2$. It is easy to determine that $A_1+ A_2$ is still a subspace of $X$.

Notice that the decomposition is not determined uniquely.  
Let $x=x_1+x_2=x_1'+x_2'$, then $x_1-x_1'=x_2-x_2'=z\in A_1\cap A_2$. Only if $A_1\cap A_2=\left\{ 0 \right\}$, then $x=x_1+x_2$ is uniquely determined. In this time, we called that sum as **direct sum** of $A_1$ and $A_2$, denote as $A_1\oplus A_2$.

:::

::: {.proposition  name=""}

-   Let $A_1,A_2$ be subspaces of $X$ and let $S_1,S_2$ be systems of generators of $A_1$ and $A_2$, then $S_1\cup S_2$ generates $A_1+A_2$.
-   Suppose that $A_1\cap A_2=\left\{ 0 \right\}$ and $T_1,T_2$ are basis of $A_1,A_2$, then $T_1\cup T_2$ is the basis of $A_1\oplus A_2$.

:::

::: {.proof}

Give any $x\in A_1+A_2$, then $x=x_1+x_2$ for some $x_1\in A_1,x_2\in A_2$. $x_1=\sum_{\alpha}^{} \lambda_\alpha x_\alpha$ for some $x_\alpha\in S_1$ and $x_2=\sum_{\beta}^{} \gamma_\beta x_\beta$ for some $x_\beta\in S_2$, so $x=\sum_{\alpha}^{} \lambda_\alpha x_\alpha+\sum_{\beta}^{} \gamma_\beta x_\beta$, notice that every $x_\alpha,x_\beta\in S_1\cup S_2$, so $S_1\cup S_2$ generates $A_1+A_2$.

Now we need to prove that $T_1\cup T_2$ is linearly independent.  
Notice that $T_1\subset A_1,T_2\subset A_2$, $A_1\cap A_2=\left\{ 0 \right\}$, so $T_1\cap T_2=\left\{ 0 \right\}$. So consider $x\in A_1\oplus A_2$, $x=\sum_{\alpha}^{} \lambda_\alpha x_\alpha+\sum_{\beta}^{} \gamma_\beta x_\beta=0$, then $A_1\owns x_1=\sum_{\alpha}^{} \lambda_\alpha x_\alpha=-\sum_{\beta}^{} \gamma_\beta x_\beta=x_2\in A_2$, so $x_1=x_2=0$, then as the property of basis, $\lambda_\alpha=0$ for all $\alpha$ and $\gamma_\beta=0$ for all $\beta$.

:::





















