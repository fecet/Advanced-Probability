## Inner Product spaces

::: {.definition  name=""}

Let $X$ be a vector space, a function, $\langle \bm{\mathbf{x}}, \bm{\mathbf{y}}\rangle$, defined for all $\bm{\mathbf{x}}\in X$ and $\bm{\mathbf{y}} \in X$, is an **inner product** if for any $\bm{\mathbf{x,y,z}} \in X$ and any $c \in \mathbb{R}$:

1. $\langle \bm{\mathbf{x,x}}\rangle \ge 0$ and equality holds iff $\bm{\mathbf{x=0}}$
2. $\langle \bm{\mathbf{y,x}} \rangle=\langle \bm{\mathbf{x,y}} \rangle$
3. $\langle \bm{\mathbf{x+y,z}} \rangle=\langle \bm{\mathbf{x,z}} \rangle+\langle \bm{\mathbf{y,z}} \rangle$
4. $\langle c \bm{\mathbf{x,y}} \rangle=c \langle \bm{\mathbf{x,y}} \rangle$

:::

### Orthogonal

Two vectors are said to be **orthogonal** if $\langle \bm{\mathbf{x,y}} \rangle=0$ and denoted as $\bm{\mathbf{x}} \perp \bm{\mathbf{y}}$ and $\bm{\mathbf{x}} \perp X$ if $\bm{\mathbf{x}} \perp \bm{\mathbf{y}}$ for all $\bm{\mathbf{y}}\in X$.

As one can apply Gramâ€“Schmidt orthonormalization for a basis in a vector space equipped inner product, we have


::: {.theorem  name=""}

Every finite dimensional non-trivial vector space has an orthogonal basis.

:::


::: {.theorem  name=""}

Let $X \subset \mathbb{R}^{m}$ is a subspace with an orthogonal basis, then each $\bm{\mathbf{x}}\in \mathbb{R}^{m}$ can be expressed uniquely as $\bm{\mathbf{x=u+v}}$ where $\bm{\mathbf{u}}\in X$ and $\bm{\mathbf{u}}\perp X$

:::

Such $\bm{\mathbf{u}}$ is known as the orthogonal projection of $\bm{\mathbf{x}}$ onto $X$ and such $\bm{\mathbf{v}}$ is called **component** of $\bm{\mathbf{x}}$ orthogonal to $X$. All orthogonal components is also a vector space.


::: {.definition  name=""}

Suppose $S$ is a vector subspace of $X$ then it's orthogonal component $S^{\perp}$ is collection of all vectors $\bm{\mathbf{x}}$ in $X$ $s.t.$ $\bm{\mathbf{x}} \perp S$.

:::

One can easily check that an orthogonal component is also a vector subspace of $X$.


::: {.theorem  name=""}

$X=S \oplus S^{\perp}$

:::


## Dimension


Recall \@ref(thm:f-dim-basis), every system of generators contains a basis, so if the generators of the system is finite, there exists a finite base of the space.


::: {.definition #def-dim name="dim"}

Consider a vector space $X$ whose basis is the family of finite number of vectors i.e. $\left\{x_1,\ldots,x_n\right\}$ generates $X$ and $\sum_{i=1}^{n} \alpha_{i}x_{i}=0$ whenever $\alpha_{i}=0$ for every $i$. Then denotes the **dim of $X$** as $\mathop{\text{dim}}X=n$.

:::

::: {.proposition  name=""}

Suppose a vector space $X$ has a basis of $n$ vectors. Then every family of $(n+1)$ vectors is linearly dependent. That means $n$ is the maximum number of linearly independent vectors in $X$ and hence every basis of $X$ consists of $n$ vectors.

:::

::: {.proof}

We use mathematical induction to prove this proposition.

1.  Let $n=1$, let $x_1$ be a basis of $X$, then $y_1,y_2\neq 0$ and $y_1,y_2\in X$. Then $y_1=\alpha x,y_2=\beta x$. Now let $\gamma_1y_1+\gamma_2y_2=0$, we can let $\gamma_1=\alpha\beta,\gamma_2=-\alpha\beta$ which means $y_1,y_2$ are linearly dependent.

2.  Assume that the proposition holds for every vector space having basis of $r\le n-1$ vectors by the induction. 

3.  Let $X$ be a vector space and let $\left\{x_1,\ldots,x_n\right\}$ be the basis of $X$ and $\left\{y_1,\ldots,y_{n+1}\right\}$ be an arbitrary family of vectors in $X$.  
    Now consider the factor space $X /\mathop{\text{span}}y_{n+1}$ and the canonical projection $\pi:X\to X /\mathop{\text{span}}y_{n+1}$. As $\left\{x_{i}:i=1,\ldots,n\right\}$ generates $X$ and $\pi$ is surjective, $\left\{\pi(x_{i}):i=1,\ldots,n\right\}$ generates $X_1=X /\mathop{\text{span}}y_{n+1}$, so according to \@ref(thm:f-dim-basis), it contains a basis of $X_1$ and as $y_{n+1}=\sum_{i=1}^{n} \alpha_{i}x_{i}$ for some not all zero $\alpha_{i}$, $\left\{\overline{x_{i}}=\pi(x_{i}):i=1,\ldots,n\right\}$ is linearly dependent, so $\mathop{\text{dim}}X_1\le n-1$, then by the hypothesis of induction, $\left\{\overline{y_{i}}=\pi(y_{i}):i=1,\ldots,n\right\}$ are linearly independent.  
    so there exists:
    $$ 
    \sum_{i=1}^{n} \gamma_{i}\overline{y_{i}}=0 \mathop{\text{ for non-trivial }}\left\{\gamma_{i}\right\}
    $$
    which means $\left\{y_{i}:i=1,\ldots,n\right\}$ are linearly dependent mod $\mathop{\text{span}}y_{n+1}$ which means 
    $$ 
    \sum_{i=1}^{n} \gamma_{i}y_{i}=\lambda y_{n+1}
    $$
    leads to the consult that $\left\{y_1,\ldots,y_{n+1}\right\}$ are linearly dependent.

:::

Give a vector space $X$ and a subspace $A_1\subset X$, then there exists a subspace $A_2\subset X$ s.t. $A_1\oplus A_2=X$ by \@ref(prp:a). Then let $\left\{x_\alpha\right\}$ be a basis of $A_1$ and $\left\{x_\beta\right\}$ be a basis of $A_2$, notice that $\left\{x_\alpha\right\}\cap \left\{x_\beta\right\}=\varnothing$ and $\left\{x_\alpha\right\}\cup \left\{x_\beta\right\}$ generates $X$. So we easily observe that $\mathop{\text{dim}}X=\mathop{\text{dim}}A_1+\mathop{\text{dim}}A_2$ if $A_1\oplus A_2=X$.

Then according to \@ref(prp:prp-factor-space-basis), let  $\pi$ be the canonical projection, $\left\{\overline{x_\beta}=\pi(x_\beta)\right\}$ forms a basis of $X /A_1$, so $\mathop{\text{dim}}(X /A_1)=\mathop{\text{card}}\left\{\overline{x_\beta}\right\}=\mathop{\text{card}}\left\{x_\beta\right\}=\mathop{\text{dim}}A_2$. So $\mathop{\text{dim}}X=\mathop{\text{dim}}A+\mathop{\text{dim}}(X /A_1)$.

::: {.proposition  name=""}

Let $A_1,A_2\subset X$ be arbitrary subspace of $X$. Then
$$ 
\mathop{\text{dim}}A_1+\mathop{\text{dim}}A_2=\mathop{\text{dim}}(A_1+A_2)+\mathop{\text{dim}}(A_1\cap A_2)
$$

:::

::: {.proof}

Just let $\left\{x_\alpha\right\}$ be the basis of $A_1\cap A_2$ and let $\left\{y_\beta\right\},\left\{y_\gamma\right\}$ be the extending tail i.e. they don't intersect  $\left\{x_\alpha\right\}$ and $\left\{x_\alpha\right\}\cup \left\{y_\beta\right\}$ is a basis of $A_1$ and $\left\{x_\alpha\right\}\cup \left\{y_\gamma\right\}$ is a basis of $A_2$.

Let $\mathop{\text{card}}\left\{x_\alpha\right\}=\alpha,\mathop{\text{card}}\left\{y_\beta\right\}=\beta,\mathop{\text{card}}\left\{y_\gamma\right\}=\gamma$. Then $\mathop{\text{dim}}A_1=\alpha+\beta,\mathop{\text{dim}}A_2=\alpha+\gamma,\mathop{\text{dim}}(A_1\cap A_2)=\alpha$. Now we only need to show that $\left\{x_\alpha\right\}\cup \left\{y_\beta\right\}\cup \left\{y_\gamma\right\}$ generates $A_1+A_2$. It is easy to show by the definition of generators of system. And notice that they are independent with each other. Thus $\left\{x_\alpha\right\}\cup \left\{y_\beta\right\}\cup \left\{y_\gamma\right\}$ is a basis of $A_1+A_2$ which means $\mathop{\text{dim}}(A_1+A_2)=\mathop{\text{card}}(\left\{x_\alpha\right\}+\left\{y_\beta\right\}+\left\{y_\gamma\right\})=\alpha+\beta+\gamma$.

:::

## Convex sets

Convex set is a special type subset of a vector space.


::: {.definition  name=""}

A set $S\subset \mathbb{R}^{m}$ is said to be **convex** iff for any $\bm{\mathbf{x_1,x_2}}\in S$ and $0<c<1$, we have
$$
c \bm{\mathbf{x_1}}+(1-c) \bm{\mathbf{x_2}} \in S
$$

:::


::: {.proposition  name=""}

Suppose $S_1,S_2 \subset \mathbb{R}^{m}$ and convex, then so is $S_1 \cap S_2$ and $S_1+S_2$.

:::

For any set $S$, the smallest convex contains it is called **convex hull** of $S$ and denoted as $C(X)$.


::: {.theorem  name=""}

If $S$ is convex, so is $\overline{S}$ and $S^{\circ}=\overline{S}^{\circ}$

:::


::: {.lemma  name=""}

Let $S$ be a closed convex set of $\mathbb{R}^{m}$ and $\bm{\mathbf{0}}\notin S$, then there exists $\bm{\mathbf{a}}\in \mathbb{R}^{m}$ $s.t.$ $\bm{\mathbf{a'x}}>0$ for all $\bm{\mathbf{x}}\in S$.

:::


::: {.definition  name=""}

Let $S_1,S_2\in \mathbb{R}^{m}$ be convex and $S_1\cap S_2=\emptyset$. Then there exists $\bm{\mathbf{b}}\neq 0 \in \mathbb{R}^{m}$ which separate $S_1$ and $S_2$.

:::


## Matrix and linear space


::: {.definition  name=""}

Let $\bm{\mathbf{X}}$ be matrix in $\mathbb{R}^{m\times n}$. The subspace of $\mathbb{R}^{n}$ spanned by the $m$ rows of $\bm{\mathbf{X}}$ is called the **row space** of $\bm{\mathbf{X}}$ and denoted as $\mathcal{R}(\bm{\mathbf{X}})$ and that of $\mathbb{R}^{m}$ is column space and denoted as $\mathcal{C}(\bm{\mathbf{X}})$

:::

The column(row) space often equipped:

- Inner product: $\langle \bm{\mathbf{x,y}} \rangle=\bm{\mathbf{x'Ay}}$, $\bm{\mathbf{A=I}}$ usually.
- Norm: $\bm{\mathbf{\|x}}\|=\sqrt{\langle \bm{\mathbf{x,x}} \rangle}$
- Metric: $d(\bm{\mathbf{x}},\bm{\mathbf{y}})=\sqrt{\langle \bm{\mathbf{x-y,x-y}} \rangle}$

The column space of $\bm{\mathbf{X}}$ is sometimes also referred to as the **range** or **image** of $\bm{\mathbf{X}}$. Note
$$
\mathcal{C}(\bm{\mathbf{X}})=\{\bm{\mathbf{y}}:\bm{\mathbf{y=Xa}},\bm{\mathbf{a}}\in \mathbb{R}^{n}\}
$$

Clearly, the rank of $\bm{\mathbf{X}}$ is just the dimension of $\mathcal{C}(\bm{\mathbf{X}})$ and that agree with $\mathop{\text{dim}} \mathcal{C}(\bm{\mathbf{X'}})$, $i.e.$, the number of independent columns of $\bm{\mathbf{X}}$. The null space $\mathcal{N}(\bm{\mathbf{X}})$ is the orthogonal space of $\mathcal{C}(\bm{\mathbf{X'}})$.


::: {.proposition  name=""}

Let $\bm{\mathbf{A}}\in \mathbb{R}^{m\times m}$, then:

1. $\mathop{\text{rank}}\left( \bm{\mathbf{AB}} \right)\le \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right) \land \mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)$
2. $\left|\mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)-\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)\right|\le \mathop{\text{rank}}\left( \bm{\mathbf{A+B}} \right)\le \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)$
3. $\mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)=\mathop{\text{rank}}\left( \bm{\mathbf{A'}} \right)=\mathop{\text{rank}}\left( \bm{\mathbf{AA'}} \right)=\mathop{\text{rank}}\left( \bm{\mathbf{A'A}} \right)$

:::


::: {.proof}

1.  Note $\bm{\mathbf{AB}}$ can be seen as linear transformation in $\mathcal{C}(X)$ or so in $\mathcal{C}(X')$ and claim follows.
2.  Note
    $$
    \bm{\mathbf{A+B}}=\begin{bmatrix}
        \bm{\mathbf{A}} & \bm{\mathbf{B}}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{\mathbf{I}} \\ \bm{\mathbf{I}}
    \end{bmatrix}
    $$
    So property $1$ applies and conclude:
    $$
    \mathop{\text{rank}}\left( \bm{\mathbf{A+B}} \right)\le \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{A}} & \bm{\mathbf{B}}
    \end{bmatrix} \right) \le \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)
    $$
    Replace $\bm{\mathbf{A}}$ and $\bm{\mathbf{B}}$ by $\bm{\mathbf{A+B}}$ and $-\bm{\mathbf{B}}$, we have
    $$
    \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)\le \mathop{\text{rank}}\left( \bm{\mathbf{A+B}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)
    $$
    And similar result also hold for $\bm{\mathbf{B}}$ and then claim follows.
3.  It's sufficient to show $\mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)=\mathop{\text{rank}}\left( \bm{\mathbf{A'A}} \right)$ and it's enough to show
    $$
    \mathcal{N}(\bm{\mathbf{A}})=\mathcal{N}(\bm{\mathbf{A'A}})
    $$
    To see that, note $\bm{\mathbf{Ax=0}}\implies \bm{\mathbf{A'Ax=0}}$ clearly and if $\bm{\mathbf{A'Ax=0}}$ we have $\bm{\mathbf{x'A'Ax=0}}$ and thus $\bm{\mathbf{\|A'x}}\|=0$ and there must be $\bm{\mathbf{Ax=0}}$.


:::


::: {.proposition #block-rank name=""}

Let $\bm{\mathbf{A,B,C}}$ are any matrices $s.t.$ all the block matrix involved are defined. We have

1.	
    $\mathop{\text{rank}}\left( \begin{bmatrix}
    \bm{\mathbf{A}} & \bm{\mathbf{B}}
\end{bmatrix} \right) \ge \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right) \lor \mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)$
2.	$\mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{A}} & \bm{\mathbf{0}}\\
        \bm{\mathbf{0}} & \bm{\mathbf{B}}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{0}} & \bm{\mathbf{A}}\\
        \bm{\mathbf{B}} & \bm{\mathbf{0}}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)$
3. $\mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{A}} & \bm{\mathbf{0}}\\
        \bm{\mathbf{C}} & \bm{\mathbf{B}}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{C}} & \bm{\mathbf{A}}\\
        \bm{\mathbf{B}} & \bm{\mathbf{0}}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{B}} & \bm{\mathbf{C}}\\
        \bm{\mathbf{0}} & \bm{\mathbf{A}}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{\mathbf{0}} & \bm{\mathbf{A}}\\
        \bm{\mathbf{B}} & \bm{\mathbf{C}}
    \end{bmatrix} \right)\ge 
    \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)$

:::


::: {.theorem  name=""}

Let $\bm{\mathbf{B}}$ be matrix in $\mathbb{R}^{m\times n}$ and $\bm{\mathbf{A,C}}$ justify the matrix multiplication:
$$
\mathop{\text{rank}}\left( \bm{\mathbf{ABC}} \right)\ge \mathop{\text{rank}}\left( \bm{\mathbf{AB}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{BC}} \right)-\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)
$$

:::


::: {.proof}

Note by some linear transformation, we have
$$
\begin{bmatrix}
    \bm{\mathbf{B}}&\bm{\mathbf{0}}\\
    \bm{\mathbf{0}}&\bm{\mathbf{ABC}}
\end{bmatrix} \to \begin{bmatrix}
    \bm{\mathbf{B}} & \bm{\mathbf{BC}}\\
    \bm{\mathbf{AB}} & \bm{\mathbf{0}}
\end{bmatrix}
$$
and claim follows by proposition \@ref(prp:block-rank).3.


:::

Take $\bm{\mathbf{B=I}}$, we have 


::: {.corollary name=""}

If $A \in \mathbb{R}^{m\times n}, B\in \mathbb{R}^{n\times p}$

$$
\mathop{\text{rank}}\left( \bm{\mathbf{AB}} \right)\ge \mathop{\text{rank}}\left( \bm{\mathbf{A}} \right)+\mathop{\text{rank}}\left( \bm{\mathbf{B}} \right)-n
$$

:::

### Projection Matrix

On the space $\mathbb{R}^{m}$, there exist projection matrix:

::: {.proposition  name=""}

Suppose $\bm{\mathbf{Q}}$ is orthogonal matrix, then $\bm{\mathbf{Q Q'}}$ is a projection on $\mathcal{C}(\bm{\mathbf{Q}})$.

:::

Such matrix is called **projection matrix** for the space $S$(if $S=\mathcal{C}(\bm{\mathbf{Q}})$) and denoted as $\bm{\mathbf{P_S}}$. Note for fixed $S$, the orthogonal basis $\bm{\mathbf{Q}}$ can be various, the projection matrix is unique.


::: {.proposition  name=""}

Suppose $\bm{\mathbf{Q_1}}$ and $\bm{\mathbf{Q_2}}$ are orthogonal matrices, and $\mathcal{C}(\bm{\mathbf{Q_1}})=\mathcal{C}(\bm{\mathbf{Q_2}})$, then $\bm{\mathbf{Q_1Q_1'=Q_2Q_2'}}$

:::

Recall the Gram-Schmidt orthonormalization apply linear transformation on $\bm{\mathbf{X}}$ to finally get orthogonal $\bm{\mathbf{Q}}$, such process can be represented as
$$
\bm{\mathbf{Q}}=\bm{\mathbf{X}}\bm{\mathbf{A}}
$$
Note $\bm{\mathbf{I=Q'Q=A'X'XA}}$ and $\bm{\mathbf{A}}$ is full rank square matrix, we have $\bm{\mathbf{AA'=(X'X)^{-1}}}$. Consequently:
$$
\bm{\mathbf{P_X=Q Q'=X(X'X)^{-1}X'}}
$$
In fact, $\bm{\mathbf{A}}$ must be upper triangle and $\bm{\mathbf{X=QA^{-1}}}$ is the so called QR decomposition.

Note the projection matrix is symmetric and idempotent, we can show that it's precisely characterization of projection matrix:


::: {.proposition  name=""}

If $\bm{\mathbf{P}}$ is symmetric and idempotent, then there is a vector space $X$ has $\bm{\mathbf{P}}$ as projection matrix, and $\mathop{\text{dim}} X=\mathop{\text{rank}}\left( \bm{\mathbf{P}} \right)$.

:::

:::: {.proof}


::: {.lemma  name=""}

Let $\bm{\mathbf{A}}\in \mathbb{R}^{m\times n}$ with rank $r$, then there exists full rank $F \in \mathbb{R}^{m \times r}$ and $G \in \mathbb{R}^{r\times n}$ $s.t.$ $\bm{\mathbf{A=FG}}$.

:::

By above lemma, we have $\bm{\mathbf{P=FG}}$, since $\bm{\mathbf{P}}$ is idempotent then we have
$$
\begin{aligned}
    \bm{\mathbf{FGFG=FG}} &\implies \bm{\mathbf{F'FGFGG'=F'FGG'}}
    \\&\implies \bm{\mathbf{GF=I}}\implies \bm{\mathbf{FGF=F}}
    \\&\implies
    \bm{\mathbf{(FG)'F=G'F'F=F}}
    \\&\implies
    \bm{\mathbf{G'=(F'F)^{-1}F}}
    \\&\implies
    \bm{\mathbf{P=F(F'F)^{-1}F'}}
\end{aligned}
$$
Thus $\bm{\mathbf{P}}$ be projection on $\mathcal{C}(\bm{\mathbf{F}})$. This completes the proof.

::::

Now we extend orthogonal projection to oblique case, where $X=S \oplus T$ still but $T \neq S^{\perp}$. 


::: {.definition #angle-generalization-projection name=""}

Suppose $S \oplus T=\mathbb{R}^{m}$ and $\bm{\mathbf{x=s+t}}$ where $\bm{\mathbf{x}} \in \mathbb{R}^{m}, \bm{\mathbf{s}}\in S, \bm{\mathbf{t}}\in T$, then $\bm{\mathbf{s}}$ is called **projection** on $S$ along $T$ while $\bm{\mathbf{t}}$ is so on $T$ along $S$.

:::

Suppose $\bm{\mathbf{X}} = \begin{bmatrix}
    \bm{\mathbf{S}} &\bm{\mathbf{T}}
\end{bmatrix}$ is nonsingular where $\bm{\mathbf{S}}\in \mathbb{R}^{m \times s},\bm{\mathbf{T}}\in \mathbb{R}^{m\times t}$, we have
$$
\bm{\mathbf{X^{-1}S}}=\begin{bmatrix}
                          \bm{\mathbf{I}}	\\
                          \bm{\mathbf{0}}	\\
                      \end{bmatrix},
\bm{\mathbf{X^{-1}T}}=\begin{bmatrix}
                          \bm{\mathbf{0}}	\\
                          \bm{\mathbf{I}}	\\
                      \end{bmatrix}
$$
They are orthogonal. Thus for arbitrary $\bm{\mathbf{y}}\in \mathbb{R}^{m}$, it can be unique expressed as $\bm{\mathbf{X^{-1}Sa+X^{-1}Tb}}$. To get the oblique projection, for any $\bm{\mathbf{x}}\in \mathbb{R}^{m}$, find $\bm{\mathbf{Xy=x}}$, then
$$
\bm{\mathbf{x=Xy=X(X^{-1}Sa+X^{-1}Tb)=Sa+Tb}}
$$
The oblique projection matrix is something map $\bm{\mathbf{x}}$ to $\bm{\mathbf{Sa}}$ and denoted as $\bm{\mathbf{P_{S|T}}}$. Note we have orthogonal projection matrix $\bm{\mathbf{P}}$ map $\bm{\mathbf{y}}$ to $\bm{\mathbf{X^{-1}Sa}}$, thus
$$
\bm{\mathbf{P_{S|T}=XPX^{-1}}}=\bm{\mathbf{X}}\begin{bmatrix}
                                                  \bm{\mathbf{I}}_s	& \bm{\mathbf{0}}	\\
                                                  \bm{\mathbf{0}}	& \bm{\mathbf{0}}	\\
                                              \end{bmatrix}\bm{\mathbf{X}}
$$
Clearly, $\bm{\mathbf{P_{S|T}}}$ is still idempotent but not symmetric, unless $S \perp T$.

Another generalization of projection is define $x \perp y$ iff $\bm{\mathbf{x'Ay}}=0$, where $\bm{\mathbf{A}}$ is positive definite and so we have some invertible $\bm{\mathbf{B}}$ $s.t.$ $\bm{\mathbf{A=B'B}}$. 


::: {.definition #inner-generalization-projection  name=""}

Then for any $\bm{\mathbf{x}}\in \mathbb{R}^{m}$, suppose it can be expressed as $\bm{\mathbf{x=s+t}}$ $s.t.$ $\bm{\mathbf{s}}\in S$ and $\bm{\mathbf{s'At}}=0$, then such $\bm{\mathbf{s}}$ is the orthogonal projection onto $S$ relative $A$.

:::

We will see both generalization agree.

Let $U=\{\bm{\mathbf{z}}:\bm{\mathbf{z=Bs}}, \bm{\mathbf{s}}\in S\}$, for decomposition $\bm{\mathbf{x=s+t}}$, we have $\bm{\mathbf{Bx=Bs+Bt}}$, where
$$
\bm{\mathbf{s'B'Bt=sAt=0}}
$$
Thus $\bm{\mathbf{Bt}}\in U^{\perp}$, by the uniqueness of orthogonal projection, this generalization is also unique. And if $S=\mathcal{C}(X)$, then $U=\mathcal{C}(BX)$, thus the projection onto $U$ is:
$$
\bm{\mathbf{P=BX(X'AX)^{-1}X'B'}}
$$
which map $Bx$ to $Bs$ and that implies the projection onto $S$ relative to $\bm{\mathbf{A}}$ is:
$$
\bm{\mathbf{P}}=\bm{\mathbf{X(X'AX)^{-1}XA}}
$$

Definition \@ref(def:angle-generalization-projection) and definition \@ref(def:inner-generalization-projection) agree since  in definition \@ref(def:angle-generalization-projection) $\bm{\mathbf{X}}=\begin{bmatrix}
\bm{\mathbf{S}}	&\bm{\mathbf{T}} 	\\
\end{bmatrix}$ then $\bm{\mathbf{X^{-1}S} }\perp \bm{\mathbf{X^{-1}T}}$ and we have $\bm{\mathbf{(X^{-1}Sa)'X^{-1}Tb=a'S'X^{-1'}X^{-1}Tb=s(XX')^{-1}t=0}}$, that relate to definition \@ref(def:inner-generalization-projection) clearly. For the other direction, it's clear as $\bm{\mathbf{P_{T|S}=I-P}}$.

We can see that $\bm{\mathbf{s}}$ is the nearest with $\bm{\mathbf{x}}$, since for any $\bm{\mathbf{y}}\in S$:
$$
\begin{aligned}
    d(\bm{\mathbf{x,y}}) &= d(\bm{\mathbf{x-s,y-s}})
    \\ &= 
    \bm{\mathbf{(x-s)'A(x-s)+(s-y)'A(s-y)}}+2\bm{\mathbf{(x-s)'A(s-y)}}
    \\ &= 
    \bm{\mathbf{(x-s)'A(x-s)+(s-y)'A(s-y)}}
    \\ & \ge \bm{\mathbf{(x-s)'A(x-s)}}=d(\bm{\mathbf{x,s}})
\end{aligned}
$$





### Linear transformation

All linear mappings $\varphi:\mathbb{R}^{n} \to \mathbb{R}^{m}$ can be presented as a matrix $\bm{\mathbf{A}}\in \mathbb{R}^{m\times n}$ $s.t.$ $\varphi(\bm{\mathbf{x}})=\bm{\mathbf{Ax}}$. 







