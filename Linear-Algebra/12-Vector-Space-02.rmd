## Inner Product spaces

::: {.definition  name=""}

Let $X$ be a vector space, a function, $\langle \bm{x}, \bm{y}\rangle$, defined for all $\bm{x}\in X$ and $\bm{y} \in X$, is an **inner product** if for any $\bm{x,y,z} \in X$ and any $c \in \mathbb{R}$:

1. $\langle \bm{x,x}\rangle \ge 0$ and equality holds iff $\bm{x=0}$
2. $\langle \bm{y,x} \rangle=\langle \bm{x,y} \rangle$
3. $\langle \bm{x+y,z} \rangle=\langle \bm{x,z} \rangle+\langle \bm{y,z} \rangle$
4. $\langle c \bm{x,y} \rangle=c \langle \bm{x,y} \rangle$

:::

### Orthogonal

Two vectors are said to be **orthogonal** if $\langle \bm{x,y} \rangle=0$ and denoted as $\bm{x} \perp \bm{y}$ and $\bm{x} \perp X$ if $\bm{x} \perp \bm{y}$ for all $\bm{y}\in X$.

As one can apply Gramâ€“Schmidt orthonormalization for a basis in a vector space equipped inner product, we have


::: {.theorem  name=""}

Every finite dimensional non-trivial vector space has an orthogonal basis.

:::


::: {.theorem  name=""}

Let $X \subset \mathbb{R}^{m}$ is a subspace with an orthogonal basis, then each $\bm{x}\in \mathbb{R}^{m}$ can be expressed uniquely as $\bm{x=u+v}$ where $\bm{u}\in X$ and $\bm{u}\perp X$

:::

Such $\bm{u}$ is known as the orthogonal projection of $\bm{x}$ onto $X$ and such $\bm{v}$ is called **component** of $\bm{x}$ orthogonal to $X$. All orthogonal components is also a vector space.


::: {.definition  name=""}

Suppose $S$ is a vector subspace of $X$ then it's orthogonal component $S^{\perp}$ is collection of all vectors $\bm{x}$ in $X$ $s.t.$ $\bm{x} \perp S$.

:::

One can easily check that an orthogonal component is also a vector subspace of $X$.


::: {.theorem  name=""}

$X=S \oplus S^{\perp}$

:::


## Dimension


Recall \@ref(thm:f-dim-basis), every system of generators contains a basis, so if the generators of the system is finite, there exists a finite base of the space.


::: {.definition #def-dim name="dim"}

Consider a vector space $X$ whose basis is the family of finite number of vectors i.e. $\left\{x_1,\ldots,x_n\right\}$ generates $X$ and $\sum_{i=1}^{n} \alpha_{i}x_{i}=0$ whenever $\alpha_{i}=0$ for every $i$. Then denotes the **dim of $X$** as $\mathop{\text{dim}}X=n$.

:::

::: {.proposition  name=""}

Suppose a vector space $X$ has a basis of $n$ vectors. Then every family of $(n+1)$ vectors is linearly dependent. That means $n$ is the maximum number of linearly independent vectors in $X$ and hence every basis of $X$ consists of $n$ vectors.

:::

::: {.proof}

We use mathematical induction to prove this proposition.

1.  Let $n=1$, let $x_1$ be a basis of $X$, then $y_1,y_2\neq 0$ and $y_1,y_2\in X$. Then $y_1=\alpha x,y_2=\beta x$. Now let $\gamma_1y_1+\gamma_2y_2=0$, we can let $\gamma_1=\alpha\beta,\gamma_2=-\alpha\beta$ which means $y_1,y_2$ are linearly dependent.

2.  Assume that the proposition holds for every vector space having basis of $r\le n-1$ vectors by the induction. 

3.  Let $X$ be a vector space and let $\left\{x_1,\ldots,x_n\right\}$ be the basis of $X$ and $\left\{y_1,\ldots,y_{n+1}\right\}$ be an arbitrary family of vectors in $X$.  
    Now consider the factor space $X /\mathop{\text{span}}y_{n+1}$ and the canonical projection $\pi:X\to X /\mathop{\text{span}}y_{n+1}$. As $\left\{x_{i}:i=1,\ldots,n\right\}$ generates $X$ and $\pi$ is surjective, $\left\{\pi(x_{i}):i=1,\ldots,n\right\}$ generates $X_1=X /\mathop{\text{span}}y_{n+1}$, so according to \@ref(thm:f-dim-basis), it contains a basis of $X_1$ and as $y_{n+1}=\sum_{i=1}^{n} \alpha_{i}x_{i}$ for some not all zero $\alpha_{i}$, $\left\{\overline{x_{i}}=\pi(x_{i}):i=1,\ldots,n\right\}$ is linearly dependent, so $\mathop{\text{dim}}X_1\le n-1$, then by the hypothesis of induction, $\left\{\overline{y_{i}}=\pi(y_{i}):i=1,\ldots,n\right\}$ are linearly independent.  
    so there exists:
    $$ 
    \sum_{i=1}^{n} \gamma_{i}\overline{y_{i}}=0 \mathop{\text{ for non-trivial }}\left\{\gamma_{i}\right\}
    $$
    which means $\left\{y_{i}:i=1,\ldots,n\right\}$ are linearly dependent mod $\mathop{\text{span}}y_{n+1}$ which means 
    $$ 
    \sum_{i=1}^{n} \gamma_{i}y_{i}=\lambda y_{n+1}
    $$
    leads to the consult that $\left\{y_1,\ldots,y_{n+1}\right\}$ are linearly dependent.

:::

Give a vector space $X$ and a subspace $A_1\subset X$, then there exists a subspace $A_2\subset X$ s.t. $A_1\oplus A_2=X$ by \@ref(prp:a). Then let $\left\{x_\alpha\right\}$ be a basis of $A_1$ and $\left\{x_\beta\right\}$ be a basis of $A_2$, notice that $\left\{x_\alpha\right\}\cap \left\{x_\beta\right\}=\varnothing$ and $\left\{x_\alpha\right\}\cup \left\{x_\beta\right\}$ generates $X$. So we easily observe that $\mathop{\text{dim}}X=\mathop{\text{dim}}A_1+\mathop{\text{dim}}A_2$ if $A_1\oplus A_2=X$.

Then according to \@ref(prp:prp-factor-space-basis), let  $\pi$ be the canonical projection, $\left\{\overline{x_\beta}=\pi(x_\beta)\right\}$ forms a basis of $X /A_1$, so $\mathop{\text{dim}}(X /A_1)=\mathop{\text{card}}\left\{\overline{x_\beta}\right\}=\mathop{\text{card}}\left\{x_\beta\right\}=\mathop{\text{dim}}A_2$. So $\mathop{\text{dim}}X=\mathop{\text{dim}}A+\mathop{\text{dim}}(X /A_1)$.

::: {.proposition  name=""}

Let $A_1,A_2\subset X$ be arbitrary subspace of $X$. Then
$$ 
\mathop{\text{dim}}A_1+\mathop{\text{dim}}A_2=\mathop{\text{dim}}(A_1+A_2)+\mathop{\text{dim}}(A_1\cap A_2)
$$

:::

::: {.proof}

Just let $\left\{x_\alpha\right\}$ be the basis of $A_1\cap A_2$ and let $\left\{y_\beta\right\},\left\{y_\gamma\right\}$ be the extending tail i.e. they don't intersect  $\left\{x_\alpha\right\}$ and $\left\{x_\alpha\right\}\cup \left\{y_\beta\right\}$ is a basis of $A_1$ and $\left\{x_\alpha\right\}\cup \left\{y_\gamma\right\}$ is a basis of $A_2$.

Let $\mathop{\text{card}}\left\{x_\alpha\right\}=\alpha,\mathop{\text{card}}\left\{y_\beta\right\}=\beta,\mathop{\text{card}}\left\{y_\gamma\right\}=\gamma$. Then $\mathop{\text{dim}}A_1=\alpha+\beta,\mathop{\text{dim}}A_2=\alpha+\gamma,\mathop{\text{dim}}(A_1\cap A_2)=\alpha$. Now we only need to show that $\left\{x_\alpha\right\}\cup \left\{y_\beta\right\}\cup \left\{y_\gamma\right\}$ generates $A_1+A_2$. It is easy to show by the definition of generators of system. And notice that they are independent with each other. Thus $\left\{x_\alpha\right\}\cup \left\{y_\beta\right\}\cup \left\{y_\gamma\right\}$ is a basis of $A_1+A_2$ which means $\mathop{\text{dim}}(A_1+A_2)=\mathop{\text{card}}(\left\{x_\alpha\right\}+\left\{y_\beta\right\}+\left\{y_\gamma\right\})=\alpha+\beta+\gamma$.

:::

## Convex sets

Convex set is a special type subset of a vector space.


::: {.definition  name=""}

A set $S\subset \mathbb{R}^{m}$ is said to be **convex** iff for any $\bm{x_1,x_2}\in S$ and $0<c<1$, we have
$$
c \bm{x_1}+(1-c) \bm{x_2} \in S
$$

:::


::: {.proposition  name=""}

Suppose $S_1,S_2 \subset \mathbb{R}^{m}$ and convex, then so is $S_1 \cap S_2$ and $S_1+S_2$.

:::

For any set $S$, the smallest convex contains it is called **convex hull** of $S$ and denoted as $C(X)$.


::: {.theorem  name=""}

If $S$ is convex, so is $\overline{S}$ and $S^{\circ}=\overline{S}^{\circ}$

:::


::: {.lemma  name=""}

Let $S$ be a closed convex set of $\mathbb{R}^{m}$ and $\bm{0}\notin S$, then there exists $\bm{a}\in \mathbb{R}^{m}$ $s.t.$ $\bm{a'x}>0$ for all $\bm{x}\in S$.

:::


::: {.definition  name=""}

Let $S_1,S_2\in \mathbb{R}^{m}$ be convex and $S_1\cap S_2=\emptyset$. Then there exists $\bm{b}\neq 0 \in \mathbb{R}^{m}$ which separate $S_1$ and $S_2$.

:::


## Matrix and linear space


::: {.definition  name=""}

Let $\bm{X}$ be matrix in $\mathbb{R}^{m\times n}$. The subspace of $\mathbb{R}^{n}$ spanned by the $m$ rows of $\bm{X}$ is called the **row space** of $\bm{X}$ and denoted as $\mathcal{R}(\bm{X})$ and that of $\mathbb{R}^{m}$ is column space and denoted as $\mathcal{C}(\bm{X})$

:::

The column(row) space often equipped:

- Inner product: $\langle \bm{x,y} \rangle=\bm{x'Ay}$, $\bm{A=I}$ usually.
- Norm: $\bm{\|x}\|=\sqrt{\langle \bm{x,x} \rangle}$
- Metric: $d(\bm{x},\bm{y})=\sqrt{\langle \bm{x-y,x-y} \rangle}$

The column space of $\bm{X}$ is sometimes also referred to as the **range** or **image** of $\bm{X}$. Note
$$
\mathcal{C}(\bm{X})=\{\bm{y}:\bm{y=Xa},\bm{a}\in \mathbb{R}^{n}\}
$$

Clearly, the rank of $\bm{X}$ is just the dimension of $\mathcal{C}(\bm{X})$ and that agree with $\mathop{\text{dim}} \mathcal{C}(\bm{X'})$, $i.e.$, the number of independent columns of $\bm{X}$. The null space $\mathcal{N}(\bm{X})$ is the orthogonal space of $\mathcal{C}(\bm{X'})$.


::: {.proposition  name=""}

Let $\bm{A}\in \mathbb{R}^{m\times m}$, then:

1. $\mathop{\text{rank}}\left( \bm{AB} \right)\le \mathop{\text{rank}}\left( \bm{A} \right) \land \mathop{\text{rank}}\left( \bm{B} \right)$
2. $\left|\mathop{\text{rank}}\left( \bm{A} \right)-\mathop{\text{rank}}\left( \bm{B} \right)\right|\le \mathop{\text{rank}}\left( \bm{A+B} \right)\le \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)$
3. $\mathop{\text{rank}}\left( \bm{A} \right)=\mathop{\text{rank}}\left( \bm{A'} \right)=\mathop{\text{rank}}\left( \bm{AA'} \right)=\mathop{\text{rank}}\left( \bm{A'A} \right)$

:::


::: {.proof}

1.  Note $\bm{AB}$ can be seen as linear transformation in $\mathcal{C}(X)$ or so in $\mathcal{C}(X')$ and claim follows.
2.  Note
    $$
    \bm{A+B}=\begin{bmatrix}
        \bm{A} & \bm{B}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{I} \\ \bm{I}
    \end{bmatrix}
    $$
    So property $1$ applies and conclude:
    $$
    \mathop{\text{rank}}\left( \bm{A+B} \right)\le \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{A} & \bm{B}
    \end{bmatrix} \right) \le \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)
    $$
    Replace $\bm{A}$ and $\bm{B}$ by $\bm{A+B}$ and $-\bm{B}$, we have
    $$
    \mathop{\text{rank}}\left( \bm{A} \right)\le \mathop{\text{rank}}\left( \bm{A+B} \right)+\mathop{\text{rank}}\left( \bm{B} \right)
    $$
    And similar result also hold for $\bm{B}$ and then claim follows.
3.  It's sufficient to show $\mathop{\text{rank}}\left( \bm{A} \right)=\mathop{\text{rank}}\left( \bm{A'A} \right)$ and it's enough to show
    $$
    \mathcal{N}(\bm{A})=\mathcal{N}(\bm{A'A})
    $$
    To see that, note $\bm{Ax=0}\implies \bm{A'Ax=0}$ clearly and if $\bm{A'Ax=0}$ we have $\bm{x'A'Ax=0}$ and thus $\bm{\|A'x}\|=0$ and there must be $\bm{Ax=0}$.


:::


::: {.proposition #block-rank name=""}

Let $\bm{A,B,C}$ are any matrices $s.t.$ all the block matrix involved are defined. We have

1.	
    $\mathop{\text{rank}}\left( \begin{bmatrix}
    \bm{A} & \bm{B}
\end{bmatrix} \right) \ge \mathop{\text{rank}}\left( \bm{A} \right) \lor \mathop{\text{rank}}\left( \bm{B} \right)$
2.	$\mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{A} & \bm{0}\\
        \bm{0} & \bm{B}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{0} & \bm{A}\\
        \bm{B} & \bm{0}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)$
3. $\mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{A} & \bm{0}\\
        \bm{C} & \bm{B}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{C} & \bm{A}\\
        \bm{B} & \bm{0}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{B} & \bm{C}\\
        \bm{0} & \bm{A}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{0} & \bm{A}\\
        \bm{B} & \bm{C}
    \end{bmatrix} \right)\ge 
    \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)$

:::


::: {.theorem  name=""}

Let $\bm{B}$ be matrix in $\mathbb{R}^{m\times n}$ and $\bm{A,C}$ justify the matrix multiplication:
$$
\mathop{\text{rank}}\left( \bm{ABC} \right)\ge \mathop{\text{rank}}\left( \bm{AB} \right)+\mathop{\text{rank}}\left( \bm{BC} \right)-\mathop{\text{rank}}\left( \bm{B} \right)
$$

:::


::: {.proof}

Note by some linear transformation, we have
$$
\begin{bmatrix}
    \bm{B}&\bm{0}\\
    \bm{0}&\bm{ABC}
\end{bmatrix} \to \begin{bmatrix}
    \bm{B} & \bm{BC}\\
    \bm{AB} & \bm{0}
\end{bmatrix}
$$
and claim follows by proposition \@ref(prp:block-rank).3.


:::

Take $\bm{B=I}$, we have 


::: {.corollary name=""}

If $A \in \mathbb{R}^{m\times n}, B\in \mathbb{R}^{n\times p}$

$$
\mathop{\text{rank}}\left( \bm{AB} \right)\ge \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)-n
$$

:::

### Projection Matrix

On the space $\mathbb{R}^{m}$, there exist projection matrix:

::: {.proposition  name=""}

Suppose $\bm{Q}$ is orthogonal matrix, then $\bm{Q Q'}$ is a projection on $\mathcal{C}(\bm{Q})$.

:::

Such matrix is called **projection matrix** for the space $S$(if $S=\mathcal{C}(\bm{Q})$) and denoted as $\bm{P_S}$. Note for fixed $S$, the orthogonal basis $\bm{Q}$ can be various, the projection matrix is unique.


::: {.proposition  name=""}

Suppose $\bm{Q_1}$ and $\bm{Q_2}$ are orthogonal matrices, and $\mathcal{C}(\bm{Q_1})=\mathcal{C}(\bm{Q_2})$, then $\bm{Q_1Q_1'=Q_2Q_2'}$

:::

Recall the Gram-Schmidt orthonormalization apply linear transformation on $\bm{X}$ to finally get orthogonal $\bm{Q}$, such process can be represented as
$$
\bm{Q}=\bm{X}\bm{A}
$$
Note $\bm{I=Q'Q=A'X'XA}$ and $\bm{A}$ is full rank square matrix, we have $\bm{\mathbf{AA'=(X'X)^{-1}}}$. Consequently:
$$
\bm{\mathbf{P_X=Q Q'=X(X'X)^{-1}X'}}
$$
In fact, $\bm{A}$ must be upper triangle and $\bm{\mathbf{X=QA^{-1}}}$ is the so called QR decomposition.

Note the projection matrix is symmetric and idempotent, we can show that it's precisely characterization of projection matrix:


::: {.proposition  name=""}

If $\bm{P}$ is symmetric and idempotent, then there is a vector space $X$ has $\bm{P}$ as projection matrix, and $\mathop{\text{dim}} X=\mathop{\text{rank}}\left( \bm{P} \right)$.

:::

:::: {.proof}


::: {.lemma  name=""}

Let $\bm{A}\in \mathbb{R}^{m\times n}$ with rank $r$, then there exists full rank $F \in \mathbb{R}^{m \times r}$ and $G \in \mathbb{R}^{r\times n}$ $s.t.$ $\bm{A=FG}$.

:::

By above lemma, we have $\bm{P=FG}$, since $\bm{P}$ is idempotent then we have
$$
\begin{aligned}
    \bm{FGFG=FG} &\implies \bm{F'FGFGG'=F'FGG'}
    \\&\implies \bm{GF=I}\implies \bm{FGF=F}
    \\&\implies
    \bm{(FG)'F=G'F'F=F}
    \\&\implies
    \bm{\mathbf{G'=(F'F)^{-1}F}}
    \\&\implies
    \bm{\mathbf{P=F(F'F)^{-1}F'}}
\end{aligned}
$$
Thus $\bm{P}$ be projection on $\mathcal{C}(\bm{F})$. This completes the proof.

::::

Now we extend orthogonal projection to oblique case, where $X=S \oplus T$ still but $T \neq S^{\perp}$. 


::: {.definition #angle-generalization-projection name=""}

Suppose $S \oplus T=\mathbb{R}^{m}$ and $\bm{x=s+t}$ where $\bm{x} \in \mathbb{R}^{m}, \bm{s}\in S, \bm{t}\in T$, then $\bm{s}$ is called **projection** on $S$ along $T$ while $\bm{t}$ is so on $T$ along $S$.

:::

Suppose $\bm{X} = \begin{bmatrix}
    \bm{S} &\bm{T}
\end{bmatrix}$ is nonsingular where $\bm{S}\in \mathbb{R}^{m \times s},\bm{T}\in \mathbb{R}^{m\times t}$, we have
$$
\bm{\mathbf{X^{-1}S}}=\begin{bmatrix}
                          \bm{I}	\\
                          \bm{0}	\\
                      \end{bmatrix},
\bm{\mathbf{X^{-1}T}}=\begin{bmatrix}
                          \bm{0}	\\
                          \bm{I}	\\
                      \end{bmatrix}
$$
They are orthogonal. Thus for arbitrary $\bm{y}\in \mathbb{R}^{m}$, it can be unique expressed as $\bm{\mathbf{X^{-1}Sa+X^{-1}Tb}}$. To get the oblique projection, for any $\bm{x}\in \mathbb{R}^{m}$, find $\bm{Xy=x}$, then
$$
\bm{\mathbf{x=Xy=X(X^{-1}Sa+X^{-1}Tb)=Sa+Tb}}
$$
The oblique projection matrix is something map $\bm{x}$ to $\bm{Sa}$ and denoted as $\bm{\mathbf{P_{S|T}}}$. Note we have orthogonal projection matrix $\bm{P}$ map $\bm{y}$ to $\bm{\mathbf{X^{-1}Sa}}$, thus
$$
\bm{\mathbf{P_{S|T}=XPX^{-1}}}=\bm{X}\begin{bmatrix}
                                                  \bm{I}_s	& \bm{0}	\\
                                                  \bm{0}	& \bm{0}	\\
                                              \end{bmatrix}\bm{X}
$$
Clearly, $\bm{\mathbf{P_{S|T}}}$ is still idempotent but not symmetric, unless $S \perp T$.

Another generalization of projection is define $x \perp y$ iff $\bm{x'Ay}=0$, where $\bm{A}$ is positive definite and so we have some invertible $\bm{B}$ $s.t.$ $\bm{A=B'B}$. 


::: {.definition #inner-generalization-projection  name=""}

Then for any $\bm{x}\in \mathbb{R}^{m}$, suppose it can be expressed as $\bm{x=s+t}$ $s.t.$ $\bm{s}\in S$ and $\bm{s'At}=0$, then such $\bm{s}$ is the orthogonal projection onto $S$ relative $A$.

:::

We will see both generalization agree.

Let $U=\{\bm{z}:\bm{z=Bs}, \bm{s}\in S\}$, for decomposition $\bm{x=s+t}$, we have $\bm{Bx=Bs+Bt}$, where
$$
\bm{s'B'Bt=sAt=0}
$$
Thus $\bm{Bt}\in U^{\perp}$, by the uniqueness of orthogonal projection, this generalization is also unique. And if $S=\mathcal{C}(X)$, then $U=\mathcal{C}(BX)$, thus the projection onto $U$ is:
$$
\bm{\mathbf{P=BX(X'AX)^{-1}X'B'}}
$$
which map $Bx$ to $Bs$ and that implies the projection onto $S$ relative to $\bm{A}$ is:
$$
\bm{P}=\bm{\mathbf{X(X'AX)^{-1}XA}}
$$

Definition \@ref(def:angle-generalization-projection) and definition \@ref(def:inner-generalization-projection) agree since  in definition \@ref(def:angle-generalization-projection) $\bm{X}=\begin{bmatrix}
\bm{S}	&\bm{T} 	\\
\end{bmatrix}$ then $\bm{\mathbf{X^{-1}S} }\perp \bm{\mathbf{X^{-1}T}}$ and we have $\bm{\mathbf{(X^{-1}Sa)'X^{-1}Tb=a'S'X^{-1'}X^{-1}Tb=s(XX')^{-1}t=0}}$, that relate to definition \@ref(def:inner-generalization-projection) clearly. For the other direction, it's clear as $\bm{\mathbf{P_{T|S}=I-P}}$.

We can see that $\bm{s}$ is the nearest with $\bm{x}$, since for any $\bm{y}\in S$:
$$
\begin{aligned}
    d(\bm{x,y}) &= d(\bm{x-s,y-s})
    \\ &= 
    \bm{(x-s)'A(x-s)+(s-y)'A(s-y)}+2\bm{(x-s)'A(s-y)}
    \\ &= 
    \bm{(x-s)'A(x-s)+(s-y)'A(s-y)}
    \\ & \ge \bm{(x-s)'A(x-s)}=d(\bm{x,s})
\end{aligned}
$$





### Linear transformation

All linear mappings $\varphi:\mathbb{R}^{n} \to \mathbb{R}^{m}$ can be presented as a matrix $\bm{A}\in \mathbb{R}^{m\times n}$ $s.t.$ $\varphi(\bm{x})=\bm{Ax}$. 







