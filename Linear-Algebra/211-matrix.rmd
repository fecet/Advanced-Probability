## Matrix and linear space

::: {.definition  name=""}

Let $\bm{X}$ be matrix in $\mathbb{R}^{m\times n}$. The subspace of $\mathbb{R}^{n}$ spanned by the $m$ rows of $\bm{X}$ is called the **row space** of $\bm{X}$ and denoted as $\mathcal{R}(\bm{X})$ and that of $\mathbb{R}^{m}$ is column space and denoted as $\mathcal{C}(\bm{X})$

:::

The column(row) space often equip:

- Inner product: $\langle \bm{x,y} \rangle=\bm{x'Ay}$, $\bm{A=I}$ usually.
- Norm: $\bm{\|x}\|=\sqrt{\langle \bm{x,x} \rangle}$
- Metric: $d(\bm{x},\bm{y})=\sqrt{\langle \bm{x-y,x-y} \rangle}$

The column space of $\bm{X}$ is sometimes also referred to as the **range** or **image** of $\bm{X}$. Note
$$
\mathcal{C}(\bm{X})=\{\bm{y}:\bm{y=Xa},\bm{a}\in \mathbb{R}^{n}\}
$$

Clearly, the rank of $\bm{X}$ is just the dimension of $\mathcal{C}(\bm{X})$ and that agree with $\mathop{\text{dim}} \mathcal{C}(\bm{X'})$, $i.e.$, the number of independent columns of $\bm{X}$. The null space $\mathcal{N}(\bm{X})$ is the orthogonal space of $\mathcal{C}(\bm{X'})$.


::: {.proposition  name=""}

Let $\bm{A}\in \mathbb{R}^{m\times m}$, then:

1. $\mathop{\text{rank}}\left( \bm{AB} \right)\le \mathop{\text{rank}}\left( \bm{A} \right) \land \mathop{\text{rank}}\left( \bm{B} \right)$
2. $\left|\mathop{\text{rank}}\left( \bm{A} \right)-\mathop{\text{rank}}\left( \bm{B} \right)\right|\le \mathop{\text{rank}}\left( \bm{A+B} \right)\le \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)$
3. $\mathop{\text{rank}}\left( \bm{A} \right)=\mathop{\text{rank}}\left( \bm{A'} \right)=\mathop{\text{rank}}\left( \bm{AA'} \right)=\mathop{\text{rank}}\left( \bm{A'A} \right)$

:::


::: {.proof}

1.  Note $\bm{AB}$ can be seen as linear transformation in $\mathcal{C}(X)$ or so in $\mathcal{C}(X')$ and claim follows.
2.  Note
    $$
    \bm{A+B}=\begin{bmatrix}
        \bm{A} & \bm{B}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{I} \\ \bm{I}
    \end{bmatrix}
    $$
    So property $1$ applies and conclude:
    $$
    \mathop{\text{rank}}\left( \bm{A+B} \right)\le \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{A} & \bm{B}
    \end{bmatrix} \right) \le \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)
    $$
    Replace $\bm{A}$ and $\bm{B}$ by $\bm{A+B}$ and $-\bm{B}$, we have
    $$
    \mathop{\text{rank}}\left( \bm{A} \right)\le \mathop{\text{rank}}\left( \bm{A+B} \right)+\mathop{\text{rank}}\left( \bm{B} \right)
    $$
    And similar result also hold for $\bm{B}$ and then claim follows.
3.  It's sufficient to show $\mathop{\text{rank}}\left( \bm{A} \right)=\mathop{\text{rank}}\left( \bm{A'A} \right)$ and it's enough to show
    $$
    \mathcal{N}(\bm{A})=\mathcal{N}(\bm{A'A})
    $$
    To see that, note $\bm{Ax=0}\implies \bm{A'Ax=0}$ clearly and if $\bm{A'Ax=0}$ we have $\bm{x'A'Ax=0}$ and thus $\bm{\|A'x}\|=0$ and there must be $\bm{Ax=0}$.


:::


::: {.proposition #block-rank name=""}

Let $\bm{A,B,C}$ are any matrices $s.t.$ all the block matrix involved are defined. We have

1.	
    $\mathop{\text{rank}}\left( \begin{bmatrix}
    \bm{A} & \bm{B}
\end{bmatrix} \right) \ge \mathop{\text{rank}}\left( \bm{A} \right) \lor \mathop{\text{rank}}\left( \bm{B} \right)$
2.	$\mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{A} & \bm{0}\\
        \bm{0} & \bm{B}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{0} & \bm{A}\\
        \bm{B} & \bm{0}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)$
3. $\mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{A} & \bm{0}\\
        \bm{C} & \bm{B}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{C} & \bm{A}\\
        \bm{B} & \bm{0}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{B} & \bm{C}\\
        \bm{0} & \bm{A}
    \end{bmatrix} \right)=
    \mathop{\text{rank}}\left( \begin{bmatrix}
        \bm{0} & \bm{A}\\
        \bm{B} & \bm{C}
    \end{bmatrix} \right)\ge 
    \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)$

:::


::: {.theorem  name=""}

Let $\bm{B}$ be matrix in $\mathbb{R}^{m\times n}$ and $\bm{A,C}$ justify the matrix multiplication:
$$
\mathop{\text{rank}}\left( \bm{ABC} \right)\ge \mathop{\text{rank}}\left( \bm{AB} \right)+\mathop{\text{rank}}\left( \bm{BC} \right)-\mathop{\text{rank}}\left( \bm{B} \right)
$$

:::


::: {.proof}

Note by some linear transformation, we have
$$
\begin{bmatrix}
    \bm{B}&\bm{0}\\
    \bm{0}&\bm{ABC}
\end{bmatrix} \to \begin{bmatrix}
    \bm{B} & \bm{BC}\\
    \bm{AB} & \bm{0}
\end{bmatrix}
$$
and claim follows by proposition \@ref(prp:block-rank).3.


:::

Take $\bm{B=I}$, we have 


::: {.corollary name=""}

If $A \in \mathbb{R}^{m\times n}, B\in \mathbb{R}^{n\times p}$

$$
\mathop{\text{rank}}\left( \bm{AB} \right)\ge \mathop{\text{rank}}\left( \bm{A} \right)+\mathop{\text{rank}}\left( \bm{B} \right)-n
$$

:::

### Projection Matrix

On the space $\mathbb{R}^{m}$, there exist projection matrix:

::: {.proposition  name=""}

Suppose $\bm{Q}$ is orthogonal matrix, then $\bm{Q Q'}$ is a projection on $\mathcal{C}(\bm{Q})$.

:::

Such matrix is called **projection matrix** for the space $S$(if $S=\mathcal{C}(\bm{Q})$) and denoted as $\bm{P_S}$. Note for fixed $S$, the orthogonal basis $\bm{Q}$ can be various, the projection matrix is unique.


::: {.proposition  name=""}

Suppose $\bm{Q_1}$ and $\bm{Q_2}$ are orthogonal matrices, and $\mathcal{C}(\bm{Q_1})=\mathcal{C}(\bm{Q_2})$, then $\bm{Q_1Q_1'=Q_2Q_2'}$

:::

Recall the Gram-Schmidt orthonormalization apply linear transformation on $\bm{X}$ to finally get orthogonal $\bm{Q}$, such process can be represented as
$$
\bm{Q}=\bm{X}\bm{A}
$$
Note $\bm{I=Q'Q=A'X'XA}$ and $\bm{A}$ is full rank square matrix, we have $\bm{\mathbf{AA'=(X'X)^{-1}}}$. Consequently:
$$
\bm{\mathbf{P_X=Q Q'=X(X'X)^{-1}X'}}
$$
In fact, $\bm{A}$ must be upper triangle and $\bm{\mathbf{X=QA^{-1}}}$ is the so called QR decomposition.

Note the projection matrix is symmetric and idempotent, we can show that it's precisely characterization of projection matrix:


::: {.proposition  name=""}

If $\bm{P}$ is symmetric and idempotent, then there is a vector space $X$ has $\bm{P}$ as projection matrix, and $\mathop{\text{dim}} X=\mathop{\text{rank}}\left( \bm{P} \right)$.

:::

:::: {.proof}


::: {.lemma  name=""}

Let $\bm{A}\in \mathbb{R}^{m\times n}$ with rank $r$, then there exists full rank $F \in \mathbb{R}^{m \times r}$ and $G \in \mathbb{R}^{r\times n}$ $s.t.$ $\bm{A=FG}$.

:::

By above lemma, we have $\bm{P=FG}$, since $\bm{P}$ is idempotent then we have
$$
\begin{aligned}
    \bm{FGFG=FG} &\implies \bm{F'FGFGG'=F'FGG'}
    \\&\implies \bm{GF=I}\implies \bm{FGF=F}
    \\&\implies
    \bm{(FG)'F=G'F'F=F}
    \\&\implies
    \bm{\mathbf{G'=(F'F)^{-1}F}}
    \\&\implies
    \bm{\mathbf{P=F(F'F)^{-1}F'}}
\end{aligned}
$$
Thus $\bm{P}$ be projection on $\mathcal{C}(\bm{F})$. This completes the proof.

::::

Now we extend orthogonal projection to oblique case, where $X=S \oplus T$ still but $T \neq S^{\perp}$. 


::: {.definition #angle-generalization-projection name=""}

Suppose $S \oplus T=\mathbb{R}^{m}$ and $\bm{x=s+t}$ where $\bm{x} \in \mathbb{R}^{m}, \bm{s}\in S, \bm{t}\in T$, then $\bm{s}$ is called **projection** on $S$ along $T$ while $\bm{t}$ is so on $T$ along $S$.

:::

Suppose $\bm{X} = \begin{bmatrix}
    \bm{S} &\bm{T}
\end{bmatrix}$ is nonsingular where $\bm{S}\in \mathbb{R}^{m \times s},\bm{T}\in \mathbb{R}^{m\times t}$, we have
$$
\bm{\mathbf{X^{-1}S}}=\begin{bmatrix}
                          \bm{I}	\\
                          \bm{0}	\\
                      \end{bmatrix},
\bm{\mathbf{X^{-1}T}}=\begin{bmatrix}
                          \bm{0}	\\
                          \bm{I}	\\
                      \end{bmatrix}
$$
They are orthogonal. Thus for arbitrary $\bm{y}\in \mathbb{R}^{m}$, it can be unique expressed as $\bm{\mathbf{X^{-1}Sa+X^{-1}Tb}}$. To get the oblique projection, for any $\bm{x}\in \mathbb{R}^{m}$, find $\bm{Xy=x}$, then
$$
\bm{\mathbf{x=Xy=X(X^{-1}Sa+X^{-1}Tb)=Sa+Tb}}
$$
The oblique projection matrix is something map $\bm{x}$ to $\bm{Sa}$ and denoted as $\bm{\mathbf{P_{S|T}}}$. Note we have orthogonal projection matrix $\bm{P}$ map $\bm{y}$ to $\bm{\mathbf{X^{-1}Sa}}$, thus
$$
\bm{\mathbf{P_{S|T}=XPX^{-1}}}=\bm{X}\begin{bmatrix}
                                                  \bm{I}_s	& \bm{0}	\\
                                                  \bm{0}	& \bm{0}	\\
                                              \end{bmatrix}\bm{X}
$$
Clearly, $\bm{\mathbf{P_{S|T}}}$ is still idempotent but not symmetric, unless $S \perp T$.

Another generalization of projection is define $x \perp y$ iff $\bm{x'Ay}=0$, where $\bm{A}$ is positive definite and so we have some invertible $\bm{B}$ $s.t.$ $\bm{A=B'B}$. 


::: {.definition #inner-generalization-projection  name=""}

Then for any $\bm{x}\in \mathbb{R}^{m}$, suppose it can be expressed as $\bm{x=s+t}$ $s.t.$ $\bm{s}\in S$ and $\bm{s'At}=0$, then such $\bm{s}$ is the orthogonal projection onto $S$ relative $A$.

:::

We will see both generalization agree.

Let $U=\{\bm{z}:\bm{z=Bs}, \bm{s}\in S\}$, for decomposition $\bm{x=s+t}$, we have $\bm{Bx=Bs+Bt}$, where
$$
\bm{s'B'Bt=sAt=0}
$$
Thus $\bm{Bt}\in U^{\perp}$, by the uniqueness of orthogonal projection, this generalization is also unique. And if $S=\mathcal{C}(X)$, then $U=\mathcal{C}(BX)$, thus the projection onto $U$ is:
$$
\bm{\mathbf{P=BX(X'AX)^{-1}X'B'}}
$$
which map $Bx$ to $Bs$ and that implies the projection onto $S$ relative to $\bm{A}$ is:
$$
\bm{P}=\bm{\mathbf{X(X'AX)^{-1}XA}}
$$

Definition \@ref(def:angle-generalization-projection) and definition \@ref(def:inner-generalization-projection) agree since  in definition \@ref(def:angle-generalization-projection) $\bm{X}=\begin{bmatrix}
\bm{S}	&\bm{T} 	\\
\end{bmatrix}$ then $\bm{\mathbf{X^{-1}S} }\perp \bm{\mathbf{X^{-1}T}}$ and we have $\bm{\mathbf{(X^{-1}Sa)'X^{-1}Tb=a'S'X^{-1'}X^{-1}Tb=s(XX')^{-1}t=0}}$, that relate to definition \@ref(def:inner-generalization-projection) clearly. For the other direction, it's clear as $\bm{\mathbf{P_{T|S}=I-P}}$.

We can see that $\bm{s}$ is the nearest with $\bm{x}$, since for any $\bm{y}\in S$:
$$
\begin{aligned}
    d(\bm{x,y}) &= d(\bm{x-s,y-s})
    \\ &= 
    \bm{(x-s)'A(x-s)+(s-y)'A(s-y)}+2\bm{(x-s)'A(s-y)}
    \\ &= 
    \bm{(x-s)'A(x-s)+(s-y)'A(s-y)}
    \\ & \ge \bm{(x-s)'A(x-s)}=d(\bm{x,s})
\end{aligned}
$$

### Linear transformation

All linear mappings $\varphi:\mathbb{R}^{n} \to \mathbb{R}^{m}$ can be presented as a matrix $\bm{A}\in \mathbb{R}^{m\times n}$ $s.t.$ $\varphi(\bm{x})=\bm{Ax}$. 







