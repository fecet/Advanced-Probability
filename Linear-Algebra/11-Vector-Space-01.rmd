# Vector Space

## Linear independence and basis

::: {.definition  name="linear independence"}

A family of vectors $\left\{ x_{i} \right\}_{i\in I}$ is called **linear independent** if the vectors $x_{i}$ are linearly independent i.e. 
$$ 
\sum_{i\in I}^{} \alpha_{i}x_{i}=0\implies \alpha_{i}=0\ \mathop{\text{ for each }}i
$$

:::

::: {.definition  name="system of generators"}

A subset $S\subset E$ is called a system of generators of $E$ if every vector $x\in E$ is a linear combination of vectors in $S$.

:::

::: {.proposition  name=""}

1.  Every finitely generated non-trivial vector space has a finite basis.
2.  Suppose that $S=\left\{ x_1,\ldots,x_m \right\}$ is a finite system of generators of $E$ and that the subset $R\subset S$ by $R=\left\{ x_1,\ldots,x_r \right\}\left(r\le m\right)$ consists of linearly independent vectors. Then there exists a basis $T$ of $E$ s.t. $R\subset T\subset S$.

:::

::: {.proof}

Just need to notice that every basis is the system of generators, and it is a minimal one.

:::

::: {.theorem #f-dim-basis name=""}

Let $E$ be a non-trivial vector space. Suppose $S$ is a system of generators and $R$ is a family of linearly independent vectors in $E$ s.t. $R\subset S$. Then there exists a basis $T$ of $E$ s.t. $R\subset T\subset S$.

:::

::: {.proof}

Consider the partially order defined between $R$ and $S$, find some $X\subset E$ s.t. 

-   $R\subset X\subset S$ 
-   the vectors in $X$ are linearly independent.

We note this partially order as $\mathcal{P}\left(R,S\right)$.  
Notice that for every chain $\left\{ X_\alpha \right\}\subset \mathcal{P}\left(R,S\right)$ has a maximal element $A=\bigcup_{\alpha} X_\alpha$. It is obvious that $A\in  \mathcal{P}\left(R,S\right)$ (Notice that $R\subset A\subset S$ and the property of a chain of the set that contains linearly independent vectors.)  
So we prove that every chain $\left\{ X_\alpha \right\}\subset \mathcal{P}\left(R,S\right)$ has a upper bound in $\mathcal{P}\left(R,S\right)$, so Zorn's Lemma implies that there exists a maximal element $T\in \mathcal{P}\left(R,S\right)$ s.t. vectors in $T$ are linearly independent.  
Then we just need to show that $T$ generates $E$. Give $x\in E$, suppose that $x$ is linearly independent to vectors in $T$. Notice that  $S$ generates $E$, so 
$$ 
x=\sum_{i\in I'}^{} \alpha_i x_{i} \qquad \mathop{\text{for some }}x_{i}\in S
$$
If $x$ is linearly independent to vectors in $T$ then exists some $i\in I'$ s.t. $x_{i}$ is linearly independent to vectors in $T$ and note this set as $\left\{ x_{j} \right\}_{j\in J}\subset S$, consider the set $\left\{ x_{j} \right\}_{j\in J}\cup T\supsetneqq T$ which leads to a contradiction of the maxmality of $T$. So $T$ is a basis of $E$.

:::

::: {.corollary #base-extend name=""}

1.  Every system of generators of $E$ contains a basis. In particular, every non-trivial vector space has a basis.
2.  Every family of linearly independent vectors of $E$ can be extended to a basis.

:::

## Free vector space

Let $X$ be an arbitrary set and consider all maps $f:X\to \mathbb{K}$ s.t. $f\left(x\right)\neq 0$ only for finitely many $x\in X$, denoting the set of these maps by $F\left(X\right)$, it is easy to show that $F\left(X\right)$ is a vector space.

Now give a basis of $F\left(X\right)$. For any $a\in X$, let $f_a$ be:
$$ 
f_a\left(x\right)=
\begin{cases}
    1 &x=a \\
    0 &x\neq a
\end{cases}
$$
Then $\left\{ f_a \right\}_{a\in X}$ forms a basis of $F\left(X\right)$.

$F\left(X\right)$ is called the **free vector space over $X$**.

## Linear mappings


::: {.definition  name="linear mapping"}

Suppose that $E$ and $F$ are vector spaces, and let $\varphi:E\to F$ be a set mapping s.t. 
$$ 
\varphi\left(x+y\right)=\varphi\left(x\right)+\varphi\left(y\right) \mathop{\text{ for all }}x,y\in E
$$
and 
$$ 
\varphi\left(\alpha x\right)=\alpha\varphi\left(x\right) \mathop{\text{ for all }}\alpha\in \mathbb{K}, x\in E
$$
Then we call the mapping $\varphi$ satisfying above conditions linear mappings.  
Moreover, if $F=\mathbb{K}$, then we called $\varphi$ a **linear function** on $E$.

:::

::: {.corollary  name=""}

Linear mappings preserve linear relations.

:::

::: {.proof}

Suppose $\varphi$ be a linear mappings, and let $u=\alpha x+\beta y\in E$, then
$$ 
\varphi\left(u\right)=\varphi\left(\alpha x+\beta y\right)=\alpha \varphi\left(x\right)+\beta\varphi\left(y\right)
$$

:::

Let $\varphi:E\to F, \psi:F\to G$ be linear mappings, then the composition of them $\psi \circ \varphi:E\to G$ is defined by:
$$ 
\left(\psi \circ \varphi\right)\left(x\right)=\psi \left(\varphi\left(x\right)\right)
$$
It is easy to show that $\psi \circ \varphi$ is still a linear mapping.


::: {.proposition  name=""}

Suppose $S$ is a system of generators of $E$ and $\varphi_0:S\to F$ where $F$ is also a vector space. Then $\varphi_0$ can be extended in at most one way to linear mapping $\varphi:E\to F$.  
And the extension exists iff such an extension is that 
$$ 
\sum_{i}^{} \alpha_{i}\varphi_0\left(x_{i}\right)=0
$$
whenever $\sum_{i}^{} \alpha_{i}x_{i}=0$.

:::

::: {.proof}

-
    $\implies$ : Suppose $\varphi$ to be a linear mapping and it is the extension of $\varphi_0$, then $\varphi\left(\sum_{i=1}^{n} \alpha_{i}x_{i}\right)=\sum_{i=1}^{n} \alpha_{i}\varphi\left(x_{i}\right)$ for each $x_{i}\in E$.  
    And for each $x_{i}\in S$, 
    $$ 
    \varphi\left(\sum_{i=1}^{n} \alpha_{i}x_{i}\right)=\sum_{i=1}^{n} \alpha_{i}\varphi\left(x_{i}\right)=\sum_{i=1}^{n} \alpha_{i}\varphi_0\left(x_{i}\right)
    $$
    so $\varphi\left(0\right)=\varphi_0\left(0\right)=0$.

-
    $\Longleftarrow$ : For any $x\in E$, define there exists some $\left\{ x_{i} \right\}_{i\in I}\subset S$ s.t. $x=\sum_{i\in I}^{} \alpha_{i}x_{i}$. Define
    $$ 
    \varphi\left(x\right)=\sum_{i\in I}^{} \alpha_{i}\varphi_0\left(x_{i}\right)
    $$
    It is obvious that $\varphi$ is that linear mapping.

:::

Notice that if $S$ is a basis of $E$, let $\varphi_0$ be a set map from $S$ to $E$, then $\varphi_0$ can be extended in a unique way to a linear mapping $\varphi:E\to F$.

::: {.proposition  name=""}

Let $\varphi:E\to F$ be a linear mapping and $\left\{ x_\alpha \right\}$ be a basis of $E$. Then $\varphi$ is a linear isomorphism iff the vectors $y_\alpha=\varphi\left(x_\alpha\right)$ form a basis for $F$.

:::

::: {.proof}

-   
    $\implies$:
    As $\varphi$ is a linear isomorphism, so for any $y\in F$, there exists a unique $x\in E$ s.t. $x=\varphi^{-1}\left(y\right)$. Notice that $\left\{ x_\alpha \right\}$ is a basis, so $x=\sum_{\alpha}^{} a_{\alpha}x_{\alpha}$ for some $a_\alpha$, so $y=\varphi\left(x\right)=\varphi\left(\sum_{\alpha}^{} a_\alpha x_\alpha\right)=\sum_{\alpha}^{} a_\alpha\varphi\left(x_\alpha\right)$. That means $\left\{ \varphi\left(x_\alpha\right) \right\}$ generates $F$. Then we need to prove the linear independence.  
    Let $\sum_{\alpha}^{} \lambda_\alpha x_\alpha=0$, then $\lambda_\alpha=0$ for each $\alpha$. Then let $\sum_{\alpha}^{} \gamma_\alpha\varphi\left(x_\alpha\right)=0$, then 
    $$ 
    \sum_{\alpha}^{} \gamma_\alpha\varphi\left(x_\alpha\right)=\varphi\left(\sum_{\alpha}^{} \gamma_\alpha x_\alpha\right)=0
    $$
    so $\sum_{\alpha}^{} \gamma_\alpha x_\alpha=0$ which means $\gamma_\alpha=0$ for each $\alpha$. So $\left\{ \varphi\left(x_\alpha\right) \right\}$ is a basis of $F$.

-
    $\Longleftarrow$ :
    Let $\left\{ y_\alpha=\varphi\left(x_\alpha\right) \right\}$ be a basis of $F$, then for each $y\in F$, there exists a unique components $\left(\lambda_\alpha\right)$ s.t. $\sum_{\alpha}^{} \lambda_\alpha y_\alpha=y$. Then we have
    $$ 
    \sum_{\alpha}^{} \lambda_\alpha \varphi\left(x_\alpha\right)=\varphi\left(\sum_{\alpha}^{} \lambda_\alpha x_\alpha\right)=\varphi\left(x\right)
    $$
    for some unique $x\in E$.
    
:::

## Subspace and factor space

### Subspace and Sum

::: {.definition  name="Subspace"}

Let $X$ be a vector space and let $A\subset X$ be a subset of $X$. Then $A$ is called a subspace if $A$ is also a vector space.

Let $S$ be a non-empty subset of $X$ and there exists a set, noting as $X_S$, is the linear combination of any vectors in $S$, $X_S$ is truly a subspace which is called **the subspace generated by $S$** or **linear closure** of $S$.

:::

::: {.proposition  name=""}

Let $A_1,A_2$ be two subspaces of the vector space $X$ and suppose that $A_1\cap A_2\neq \varnothing$ then $A_1\cap A_2$ is still a subspace of $X$.

:::

::: {.proof}

Notice that if $x\in A_1\cap A_2$, then $x\in A_1$ and $x\in A_2$, and $A_1,A_2$ are vector space thus provide the linearity of $A_1\cap A_2$.

:::


::: {.definition  name="sum of subspace"}

Let $A_1,A_2$ be two subspaces of a vector space $X$, then $\left\{ x=x_1+x_2:x_1\in A_1,x_2\in A_2 \right\}$ is called the **sum of $A_1$ and $A_2$**, denote as $A_1+ A_2$. It is easy to determine that $A_1+ A_2$ is still a subspace of $X$.

Notice that the decomposition is not determined uniquely.  
Let $x=x_1+x_2=x_1'+x_2'$, then $x_1-x_1'=x_2-x_2'=z\in A_1\cap A_2$. Only if $A_1\cap A_2=\left\{ 0 \right\}$, then $x=x_1+x_2$ is uniquely determined. In this time, we called that sum as **direct sum** of $A_1$ and $A_2$, denote as $A_1\oplus A_2$.

:::

::: {.proposition  name=""}

-   Let $A_1,A_2$ be subspaces of $X$ and let $S_1,S_2$ be systems of generators of $A_1$ and $A_2$, then $S_1\cup S_2$ generates $A_1+A_2$.
-   Suppose that $A_1\cap A_2=\left\{ 0 \right\}$ and $T_1,T_2$ are basis of $A_1,A_2$, then $T_1\cup T_2$ is the basis of $A_1\oplus A_2$.

:::

::: {.proof}

Give any $x\in A_1+A_2$, then $x=x_1+x_2$ for some $x_1\in A_1,x_2\in A_2$. $x_1=\sum_{\alpha}^{} \lambda_\alpha x_\alpha$ for some $x_\alpha\in S_1$ and $x_2=\sum_{\beta}^{} \gamma_\beta x_\beta$ for some $x_\beta\in S_2$, so $x=\sum_{\alpha}^{} \lambda_\alpha x_\alpha+\sum_{\beta}^{} \gamma_\beta x_\beta$, notice that every $x_\alpha,x_\beta\in S_1\cup S_2$, so $S_1\cup S_2$ generates $A_1+A_2$.

Now we need to prove that $T_1\cup T_2$ is linearly independent.  
Notice that $T_1\subset A_1,T_2\subset A_2$, $A_1\cap A_2=\left\{ 0 \right\}$, so $T_1\cap T_2=\left\{ 0 \right\}$. So consider $x\in A_1\oplus A_2$, $x=\sum_{\alpha}^{} \lambda_\alpha x_\alpha+\sum_{\beta}^{} \gamma_\beta x_\beta=0$, then $A_1\owns x_1=\sum_{\alpha}^{} \lambda_\alpha x_\alpha=-\sum_{\beta}^{} \gamma_\beta x_\beta=x_2\in A_2$, so $x_1=x_2=0$, then as the property of basis, $\lambda_\alpha=0$ for all $\alpha$ and $\gamma_\beta=0$ for all $\beta$.

:::

::: {.definition  name="complementary subspace"}

If $A_1$ is a subspace of $X$, and there exists a subspace $A_2$ s.t. $A_1\oplus A_2=E$, then $A_2$ is called the **complementary subspace** for $A_1$ in $X$.

:::

::: {.proposition #a name="existence of complementary subspace"}

If $A_1\subset X$ is a subspace, then there exists a $A_2\subset X$ a subspace s.t. $A_1\oplus A_2=X$

:::

::: {.proof}

According to the \@ref(cor:base-extend), suppose that $\left\{x_\alpha\right\}$ is a basis of $A_1$, then it is linearly independent and so can be extended to a basis of $X$, denote as $\left\{x_\gamma\right\}$. Notice that $\left\{x_\alpha\right\}\subset \left\{x_\gamma\right\}$ and let $\left\{x_\beta\right\}=\left\{x_\gamma\right\}-\left\{x_\alpha\right\}$. Then let $A_2$ be the subspace generated by $\left\{x_\beta\right\}$.

Observe that $\left\{x_\alpha\right\}\cup \left\{x_\beta\right\}$ generates $X$, so $A_1+A_2=X$, then let $x\in A_1\cap A_2$, so $x=\sum_{\alpha}^{} \lambda_\alpha x_\alpha=\sum_{\beta}^{} \omega_\beta x_\beta$ which means $\sum_{\alpha}^{} \lambda_\alpha x_\alpha+\sum_{\beta}^{} (-\omega_\beta)x_\beta=0$. For vectors in $\left\{x_\alpha\right\}$ and $\left\{x_\beta\right\}$ are linearly independent, so $\lambda_\alpha=0,\omega_\beta=0$ for all $\alpha,\beta$, then $A_1\cap A_2=\left\{0\right\}$ which means $X=A_1\oplus A_2$.

:::

::: {.corollary  name=""}

Let $A_1$ be a subspace of $X$ and $\varphi_1:A_1\to F$ be a linear mapping. Then $\varphi_1$ may be extended to a linear mapping $\varphi:X\to F$.

:::

::: {.proof}

According to the above proposition, there exists a subspace $A_2\subset X$ s.t. $A_1\oplus A_2=X$. Now define $\varphi_2:A_2\to F$ be a linear mapping. Then for any $x\in X$, notice that $x=x_1+x_2$ where $x_1\in A_1,x_2\in A_2$, define 
$$ 
\varphi(x)=\varphi_1(x_1)+\beta\varphi_2(x_2)\qquad x=x_1+x_2; \beta\in \mathbb{K}
$$
It is easy to show that $\varphi$ is a linear mapping as $\varphi_1,\varphi_2$ are.

:::

### Factor Space


::: {.definition  name="factor space"}

Suppose that $X$ is a vector space and $A_1$ is a subspace of $X$. Two vectors $x,x'\in X$ is called **equivalent** mod $A_1$ if $x-x'\in A_1$. Then $x\sim x'$ is a equivalence relation, that is reflexive, symmetric and transitive.

Then we let $X /A_1$ denote the **set of equivalence classes**, $X /A_1$ is a vector space too and define a mapping:
$$ 
\pi:X\to X /A_1
$$
by letting $\pi x=\overline{x},x\in X$ where $\overline{x}$ denotes the equivalence class containing $x$. Clearly, $\pi$ is a surjective mapping.

:::

::: {.proof}

Now prove the equivalent relation:

-   let $x\sim x_1$, $x_1\sim x_2$, which means $x-x_1\in A_1$ and $x_1-x_2\in A_1$ then $x-x_2=(x-x_1)+(x_1-x_2)\in A_1$.
-   Notice that $x-x=0\in A_1$ as $A_1$ is a subspace.
-   Observe that $x-x_1=(-1)(x_1-x)$ which means the symmetry.

:::

::: {.proposition  name=""}

There exists precisely one linear structure in $X /A_1$ s.t. $\pi$ is a linear mapping.

:::

::: {.proof}

Assume that $X /A_1$ is made into a vector space s.t. $\pi$ is a linear mapping. Then
$$ 
\pi(x+y)=\pi(x)+\pi(y)
$$
and $\pi(\lambda x)=\lambda \pi(x)$. It shows that we can use a linear mapping $\pi$ to define the linear structure of $X /A_1$ and the linear structure of $X /A_1$ is determined by the linear structure of $X$, thus unique.

Now define the linear structure of $X /A_1$. Let $\overline{x},\overline{y}\in X /A_1$ and $\overline{x}\neq \overline{y}$. Then there exists some $x,y\in X$ s.t. $\pi(x)=\overline{x}$ and $\pi(y)=\overline{y}$. Pick an arbitrary $x$ and $y$, define:
$$
\overline{x}+\overline{y}=\pi(x+y)
$$
and
$$ 
\lambda \overline{x}=\pi(\lambda x)
$$
We only need to show that $\pi$ is a linear mapping. Suppose that $x_1-x_2\in A_1$ and $y_1-y_2\in A_1$, notice that $(x_1+y_1)-(x_2+y_2)=(x_1-x_2)+(y_1-y_2)\in A_1$ as the property of subspace. Since the picking of $x_1,x_2,y_1,y_2$ is arbitrary, $\pi(x)=\overline{x}$, $\pi(x+y)=\overline{x}+\overline{y}$. Then $\pi$ is a communicative group as above. Similarly, it is easy to show that $\pi(\lambda x)=\lambda \pi(x)$. Then $\pi$ is linear, so it determines the linear structure of $X /A_1$.

:::

::: {.remark  name=""}

The space discussed above like $X /A_1$ is called the factor space or quotient space and the linear mapping $\pi:X\to X /A_1$ is called the canonical projection of $X$ onto $A_1$.

:::

::: {.definition  name=""}

Let $A_1$ be a subspace of $X$, and suppose $\left\{x_\alpha\right\}$ is a family of vectors in $X$. Then $x_\alpha$ is called **linear dependent mod $A_1$** if there are scalars $\lambda_\alpha$ not all zero s.t. $\sum_{\alpha}^{} \lambda_\alpha x_\alpha \in A_1$.

A family of vectors is called linearly independent mod a subspace $A_1$ if they are not linearly dependent mod $A_1$.

:::

Now consider the canonical projection $\pi:X\to X /A_1$, then $\left\{x_\alpha\right\}$ is linearly dependent mod $A_1$ iff the vectors $\pi(x_\alpha)$ are linearly dependent in $X /A_1$.

::: {.proof}

-
    $\implies$ :    Suppose $\left\{x_\alpha\right\}$ is linear dependent mod $A_1$, then $\sum_{\alpha}^{} \lambda_\alpha x_\alpha\in A_1$ for not all zero $\lambda_\alpha$, notice that the linearity of $\pi$, 
    $$
    \sum_{\alpha}^{} \lambda_\alpha \pi(x_\alpha)=\pi\left(\sum_{\alpha}^{} \lambda_\alpha x_\alpha\right)
    $$
    Observe that $\sum_{\alpha}^{} \lambda_\alpha x_\alpha=x\in A_1$, and only if $x\in A_1$, $\pi(x)=\overline{0}$ in $X /A_1$.

-
    $\Longleftarrow$ : Omission.

:::

Suppose that $\left\{x_\alpha\right\}\cup \left\{x_\beta\right\}$ is a basis of $X$ and $\left\{x_\alpha\right\}$ generates $A_1$, then according to \@ref(prp:a) there exists a $A_2$ generated by $\left\{x_\beta\right\}$ s.t. $A_1\oplus A_2=X$.

::: {.proposition #prp-factor-space-basis name="basis of a factor space"}

$\pi(x_\beta)$ for all $\beta$ form a basis of $X /A_1$.

:::

::: {.proof}

First, we need to prove that $\pi(x_\beta)$ generates $X /A_1$.  
Let $\overline{x}\in X /A_1$ be an arbitrary element. We only need to find a $x\in \pi^{-1}(\overline{x})$, notice that if $\overline{x}$ is non-trivial i.e. $\overline{x}\neq \overline{0}$, $x\notin A_1$, so there must exist some $\gamma_\beta$ s.t. $x=\sum_{\beta}^{} \gamma_\beta x_\beta$. Then
$$ 
\pi\left(\sum_{\beta}^{} \gamma_\beta x_\beta\right)=\pi(x)=\overline{x}=\sum_{\beta}^{} \gamma_\beta \pi(x_\beta)
$$

Second, we observe that $\left\{x_\beta\right\}$ is linearly independent mod $A_1$, so $\pi(x_\beta)$ are linearly independent in $X /A_1$.

:::
