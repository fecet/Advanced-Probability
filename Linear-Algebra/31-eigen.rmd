# Matrix Analysis {-}

# Eigenvalues

Suppose $\bm{A}\in \mathbb{R}^{m}$, if $\bm{Ax=}\lambda \bm{x}$, we say $\lambda$ eigenvalue of $\bm{A}$ and $\bm{x}$ is eigenvector of $\bm{A}$. To find $\lambda$, we solve following characteristic equation of $\bm{A}$:
$$
\left| \bm{A}-\lambda \bm{I}\right|=0
$$
Recall the Fundamental theorem of algebra, there is $m$ eigenvalues and the times of $\lambda$ repeated is called **algebraic multiplicity**, or multiplicity for short and denoted as $\mu_{\bm{A}}(\lambda)$.

Note the eigenvector for a eigenvalue $\lambda$ is not unique, in fact, all of them formed a vector space.

::: {.theorem  name=""}

If $S_{\bm{A}}(\lambda)$ is all eigenvectors of $\bm{A}$ corresponding to $\lambda$, then $S_{\bm{A}}(\lambda)$ is a vector space.

:::

The dimension of eigenspace of $\lambda$ is called **geometric multiplicity** of $\lambda$ and deonted as $\gamma_{\bm{A}}(\lambda)$.

Following are frequently using:


::: {.proposition #eigen-property name=""}

Let $\bm{A}\in \mathbb{R}^{m\times m}$, $\lambda$ is it's eigenvalue, then the following holds:

1. The eigenvalues of $\bm{A'}$ are the same as that of $\bm{A}$.
2. $\bm{A}$ is singular iff $0$ is a eigenvalues.
3. The eigenvalues of $\bm{\mathbf{BAB^{-1}}}$ are the same as $\bm{A}$.
4. If $\bm{A}$ is orthogonal, $|\lambda_i|=1$.
5. $1\le \gamma_{\bm{A}}(\lambda)\le \mu_{\bm{A}}(\lambda)\le m$.
6. $\lambda^{n}$ is an eigenvalue of $\bm{A}^{n}$ and the eigenspace remain the same, where $n$ can be negative when $\bm{A}$ is invertible.
7. $\mathop{\text{tr}}\left( \bm{A} \right)=\sum_{i=1}^{m}\lambda_i$, $\left| \bm{A}\right|=\prod_{i=1}^{m} \lambda_i$.
8. $\sigma_{\bm{AB}}=\sigma_{\bm{BA}}$ if ignore zero eigenvalues.

:::


:::: {.proof}

**7**. Recall the characteristic equation of the form:
$$
(-\lambda)^{m}+\alpha_{m-1}(-\lambda)^{m-1}+\dots+\alpha_1(-\lambda)+\alpha_0=0
$$
By the Vieta's formulas,
$$
\sum_{i=1}^{m}\lambda_i=\alpha_{m-1},\prod_{i=1}^{n} \lambda_i=\alpha_0
$$
For $\alpha_{m-1}$, by the definition of determinant, it comes from term $\prod_{i=1} ^{m} (a_{ii}-\lambda)$ and thus equal to $\sum_{i=1}^{m}a_{ii}=\mathop{\text{tr}}\left( \bm{A}\right)$. For $\alpha_0$, let $\lambda=0$ in above equation and we have $\left| \bm{A}\right|=\alpha_0$. This completes the proof.



::::


::: {.proposition  name=""}

Let $\bm{A}\in \mathbb{R}^{m\times m}$ and symmetric, $\bm{c,d} \in \mathbb{R}^{m}$, then
$$
\left| \bm{A+cd'}\right|=\left| \bm{A}\right|(1+\bm{\mathbf{d'A^{-1}c}})
$$

:::

:::: {.proof}

$$
\left| \bm{A+cd'} \right|=\left| \bm{\mathbf{A(I+A^{-1}cd')}} \right|=\left| \bm{A} \right|\left| \bm{\mathbf{I+A^{-1}cd'}} \right|=\left|  \bm{A}\right|(1+\bm{\mathbf{c'A^{-1'}d}})=\left| \bm{A} \right|(1+\bm{\mathbf{d'A^{-1}c}})
$$
where we use the truth:

::: {.lemma  name=""}

$\left| \bm{I+bd'} \right|=1+\bm{d'b}$

:::

Since for any orthogonal vector $\bm{x}$ to $\bm{d}$, $(\bm{I+bd'})\bm{x}=\bm{x}$, they are eigenvectors of $1$ and thus $\mu_{\bm{A}}(1)\ge \gamma_{\bm{A}}(1)=m-1$. Notice $\mathop{\text{tr}}\left( \bm{I+bd'} \right)=m+\bm{d'b}$ and that implies there are exactly $1$ eigenvalues is $1+\bm{d'b}$ and claim follows by compute $\prod_{} \lambda_i$.

::::


::: {.proposition  name=""}

Suppose $\bm{x_1,x_2,\dots,x_r}$ belong to different $\lambda_i$, then they are linearly independent.

:::

Suppose $\mathop{\text{eig}}\left(\bm{A}\right)$ are all distinct, then let
$$
\bm{X}=\begin{bmatrix}
                    \bm{x_1}	& 	\dots& 	\bm{x_m}\\
                \end{bmatrix}
$$
where $\bm{x}_i$ is an eigenvector corresponding to $\lambda_i$. Then $\bm{Ax_i}=\lambda_i \bm{x_i}$ implies $\bm{AX=X}\mathop{\text{diag}}(\lambda_i)$. That is, $\bm{\mathbf{A=X\Lambda X^{-1}}}$ is **diagonalizable**. If $\bm{A}$ is diagonalizable, then it's rank is the number of its nonzero eigenvalues, also, in view of proposition \@ref(prp:eigen-property), $\mu_{\bm{A}}(\lambda)=\gamma_{\bm{A}}(\lambda)$.

The following theorem stats that a matrix satisfy its own characteristic equation.

::: {.theorem #cayley-hamilton name="Cayley-Hamilton"}

Suppose $\mathop{\text{eig}}\left(\bm{A}\right)=\lambda_1, \dots, \lambda_m$ then
$$
\prod_{i=1} ^{m} \bm{A}-\lambda_i \bm{I}=\bm{0}
$$

:::

## Symmetric matrices and Spectral Decomposition

Symmetric matrices avoid occurrence of complex eigenvalues:


::: {.theorem  name=""}

Let $\bm{A} \in \mathbb{R}^{m\times m}$ be symmetric, then all eigenvalues of $\bm{A}$ are real.

:::


:::: {.proof}

Suppose $\lambda \in \mathop{\text{eig}}\left(\bm{A}\right)$, then
$$
\bm{(Ax)^*x}=\overline{\lambda} \bm{x}^*\bm{x}
$$
on the other hand
$$
\bm{\mathbf{(Ax)^{*}x}}=\bm{x^*Ax}=\lambda \bm{x^*x}
$$
thus $\overline{\lambda}=\lambda$ and must be real.

::::


::: {.remark}

The real eigenvalues suggest real eigenvector existence, suppose $\bm{x=a+}i \bm{b}$, then
$$
\bm{Ax}=\bm{Aa}+i \bm{Ab}=\lambda \bm{a}+i \lambda \bm{b}
$$
thus $\bm{a}$ is also eigenvector.


:::

We have see that sets of eigenvectors comes from different eigenvalues are linearly independent. If $\bm{A}$ is symmetric, they are even orthogonal. Suppose $\lambda,\gamma \in \sigma_{\bm{A}}$ and $\lambda \neq \gamma$, corresponding to eigenvectors $\bm{x}$ and $\bm{y}$.
$$
\begin{aligned}
    \lambda \bm{x'y}&=(\lambda \bm{x})\bm{'y} = (\bm{Ax})' \bm{y}=\bm{x'A'y}
    \\ &= 
    \bm{x'} \gamma \bm{y}=\gamma \bm{x'y}\implies \bm{x'y}=0
\end{aligned}
$$
Thus, if all the $m$ eigenvalues are distinct, Spectral decomposition can be applied. In fact, it's possible even $\bm{A}$ has multiple eigenvalues. To see this, we need following theorem.


::: {.lemma  name=""}

Let $\bm{A}\in \mathbb{R}^{m\times m}$ be symmetric and $\bm{x}\in \mathbb{R}^{m}$, then there is some $\lambda_i \in \sigma_{\bm{A}}$ $s.t.$
$$
\lambda_i \in \mathop{\text{span}}\left( \bm{\mathbf{x,Ax,\dots,A^{r-1}x}} \right)
$$
for some $r \ge 1$

:::


:::: {.proof}

Let $r$ be the smallest for which $\left( \bm{\mathbf{x,Ax,\dots,A^{r}x}} \right)$ are linearly dependent. Then there exist not all zero $\alpha_i$ $s.t.$:
$$
\alpha_0 \bm{x}+\alpha_1 \bm{Ax}+\dots+\alpha_r \bm{\mathbf{A^{r}x}}=(\alpha_0 \bm{I}+\alpha_1 \bm{A}+\dots+\bm{A^r})\bm{x}=\bm{0}
$$
where we let $\alpha_r=0$ WLOG. By Fundamental Algebra Theorem, there exist $\gamma_i$ $s.t.$
$$
\sum_{i=0}^{r} \alpha_i \bm{A}^{i}=\prod_{i=1} ^{m} (\bm{A-}\gamma_i \bm{I})
$$
Now let $\bm{y=}\left[\prod_{i=2} ^{m} (\bm{A}-\gamma_i \bm{I}) \right]\bm{x}$, its nonzero as $\bm{x},\bm{Ax},\dots, \bm{A}^{r-1}\bm{x}$ are linearly independent. Thus $\bm{y}$ is in $\mathop{\text{span}}\left( \bm{x},\bm{Ax},\dots, \bm{A}^{r-1}\bm{x} \right)$ and it follows that
$$
(\bm{A-}\gamma_1 \bm{I})\bm{y}=\bm{0}
$$
and then claim follows.

::::

Above lemma gives a way to find a new orthogonal eigenvector from existed $\bm{x_1,\dots,x_h}$, select $\bm{x}$ orthogonal to all of them then $\bm{\mathbf{A^{k}x}}$ remains orthogonal since
$$
\bm{\mathbf{x_i'A^{k}x}}=\bm{\mathbf{(A^{k}x_i)'x}}=\lambda_i^{k} \bm{x_i'x}=0
$$
so the vector $\bm{y}$ given by the lemma is desired. Then we can constructed a set of $m$ eigenvectors that are orthonormal.  
As we said before, then so called spectral decomposition applied. Let $\bm{Q}=(\bm{x_1},\dots,\bm{x_m})$ constructed by the orthonormal set and become an orthogonal matrix, then $\bm{A}=\bm{Q \Lambda Q'}$ where $\bm{\Lambda}=\mathop{\text{diag}}(\lambda_i)$ as before.

Clearly, in this case, geometric multiplicity and algebraic multiplicity coincide and rank is number of nonzero eigenvalues.

## Eigenprojections

A set of orthonormal eigenvectors can be used to find **eigenprojections** of $\bm{A}$.


::: {.definition  name=""}

Let $\lambda$ be an eigenvalues of symmetric $\bm{A}\in \mathbb{R}^{m\times m}$ with multiplicity $r \ge 1$, $\{\bm{x_i}\}_{1}^{r}$ be the orthonormal set of eigenvectors, then the **eigenprojections** of $\bm{A}$ is
$$
{\bm{P_A}}(\lambda)=\sum_{i=1}^{r}\bm{x_ix_i'}
$$

:::

This is orthogonal projection for eigenspace $S_{\bm{A}}(\lambda)$. Let $\{\lambda_i\}$ be the multiset of eigenvalues and $\{\mu_i\}$ be set of them, then
$$
\bm{A}=\bm{Q\Lambda Q'}=\sum_{i=1}^{m}\lambda_i \bm{x_ix_i'}=\sum_{i=1}^{k}\mu_i \bm{P_A}(\mu_i)
$$
The last term is preferred than the second since it's term are unique.

## Advanced in eigenvalues


::: {.theorem  name=""}

Let $\bm{A,B}\in \mathbb{R}^{m\times m}$ with eigenvalues $\lambda_1,\dots,\lambda_m$ and $\gamma_1,\dots,\gamma_m$. Define
$$
\begin{aligned}
    M&=\max_{ij} \left| a_{ij} \right| \lor \left| b_{ij}\right|    \\
    \delta_{}(\bm{A,B})&=\frac{1}{m} \sum_{ij}^{}\left| a_{ij}-b_{ij} \right|
\end{aligned}
$$
then
$$
\max_{i} \min_j \left| \lambda_i-\gamma_j \right|\le (m+2)M^{1-\frac{1}{m}} \delta(\bm{A,B})^{\frac{1}{m}}
$$

:::

That implies if $\bm{B_n}\to \bm{A}$ pointwise, then $\gamma \to \lambda$. 


::: {.proposition  name=""}

$\lambda_i$ is continues function of elements of $\bm{A}$.

:::


::: {.theorem  name=""}

Suppose $\bm{A}\in \mathbb{R}^{m\times m}$ is symmetric and $\lambda \in \sigma_{\bm{A}}$. Then $\bm{P_A}(\lambda)$ is a continues function of $\bm{A}$.

:::

## Quadratic form

The quadratic form is something of the form $\bm{x'Ax}$ as a function of $\bm{x\neq 0}$, where $\bm{A}\in \mathbb{R}^{m\times m}$ is symmetric. To avoid effect of scale, we often use **Rayleigh quotient**:
$$
R(x,\bm{A})=\frac{\bm{x'Ax}}{\bm{x'x}}
$$

::: {.theorem  name=""}

$R(\bm{x},\bm{A})$ take minimum in $S_{\bm{A}}(\lambda_m)$ while maximum in $S_{\bm{A}}(\lambda_1)$.

:::

Consequently, we have:

::: {.theorem #courant-fischer name="Courant–Fischer min–max theorem"}

Let $\bm{A}\in \mathbb{R}^{m\times m}$ be symmetric with eigenvalues $\lambda_1\ge \lambda_2 \ge \dots\ge \lambda_m$. For $1\le h\le m$, let $\bm{B}_h\in \mathbb{R}^{m\times (h-1)}$ and $\bm{C}_h \in \mathbb{R}^{m\times (m-h)}$ which are orthogonal. Then
$$
\lambda_h=\min_{\bm{b}_h} \max _{\bm{b}_h' \bm{x}=\bm{0}} \frac{\bm{x'ax}}{\bm{x'x}}=\max_{\bm{C}_h} \min _{\bm{C}_h' \bm{x}=\bm{0}} \frac{\bm{x'ax}}{\bm{x'x}}
$$

:::


:::: {.proof}

Let $\bm{x}_i$ be eigenvectors corresponding to $\lambda_i$. The idea is we should specify $\bm{B}_h$ and $\bm{C}_h$ to avoid $\bm{x}_i$ according the larger (and smaller) occur in the $\mathcal{N}(\bm{B'}_h)$, so we can hide them in $\mathcal{C}(\bm{B}_h)$. That is, let $\bm{B}_h$ constructed by $\{\bm{x}\}_{1}^{h-1}$ and so the next maximum is $\lambda_h$.

::::





## Nonnegative Definite Matrix


::: {.theorem  name=""}

Suppose $\bm{A}\in \mathbb{R}^{m\times m}$ is symmetric, then

1. $\bm{A}$ is positive definite iff $\lambda>0$ for all $\lambda \in \sigma_{\bm{A}}$
2. $\bm{A}$ is positive semidefinite iff $\lambda \ge 0$ for all $\lambda \in \sigma_{\bm{A}}$ and $0\in \sigma_{\bm{A}}$

:::


:::: {.proof}

By spectral decomposition, the orthogonal matrix $\bm{Q}$ span $\mathbb{R}^{m}$, thus any $\bm{x}=\bm{Qa}$ for some $\bm{a}$, then
$$
\bm{x'Ax=x'(Q\Lambda Q')x=a'\Lambda a}
$$
Then the claim follows easily.

::::

Symmetric matrix often obtained by taking $\bm{A}=\bm{TT'}$ or $\bm{TT'}$, in fact, they share positive eigenvalues.

::: {.theorem  name=""}

Let $\bm{T}\in \mathbb{R}^{m\times m}$ with rank $r$, then positive eigenvalues of $\bm{TT'}$ are the same with $\bm{T'T}$.

:::


:::: {.proof}

$\bm{Y_1}$

::::





<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
